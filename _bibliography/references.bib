% This file was created with JabRef 2.10.
% Encoding: Cp1250


@InCollection{Alvarez2009,
  Title                    = {{Sparse Convolved Gaussian Processes for Multi-output Regression}},
  Author                   = {\'{A}lvarez, M. and Lawrence, N. D.},
  Booktitle                = {Advances in Neural Information Processing Systems 21},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2009},
  Editor                   = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
  Pages                    = {57--64},

  Abstract                 = {We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network.}
}

@Article{Alvarez2013a,
  Title                    = {{Linear Latent Force Models Using Gaussian Processes.}},
  Author                   = {\'{A}lvarez, M. and Luengo, D. and Lawrence, N. D.},
  Journal                  = {IEEE Tansactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2013},
  Number                   = {11},
  Pages                    = {2693--2705},
  Volume                   = {35},

  Abstract                 = {Purely data-driven approaches for machine learning present difficulties when data are scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand, purely mechanistic approaches need to identify and specify all the interactions in the problem at hand (which may not be feasible) and still leave the issue of how to parameterize the system. In this paper, we present a hybrid approach using Gaussian processes and differential equations to combine data-driven modeling with a physical model of the system. We show how different, physically inspired, kernel functions can be developed through sensible, simple, mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from motion capture, computational biology, and geostatistics.},
  Doi                      = {10.1109/TPAMI.2013.86},
  Keywords                 = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer Simulation,Linear Models,Normal Distribution,Pattern Recognition,Sample Size},
  Pmid                     = {24051729}
}

@Article{Alvarez2009a,
  Title                    = {{Latent Force Models}},
  Author                   = {\'{A}lvarez, M. and Luengo, D. and Lawrence, N. D.},
  Journal                  = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},
  Pages                    = {9--16},
  Volume                   = {5}
}

@Article{Astrom1971,
  Title                    = {{System Identification—A Survey}},
  Author                   = {\AA str\"{o}m, K. J. and Eykhoff, P.},
  Journal                  = {Automatica},
  Year                     = {1971},
  Number                   = {2},
  Pages                    = {123--162},
  Volume                   = {7},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/\AA str\"{o}m, Eykhoff, System Identification—A Survey, 1971.pdf:pdf},
  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0005109871900598}
}

@Article{Simandl2009,
  Title                    = {{Derivative-free estimation methods: New results and performance analysis}},
  Author                   = {\v{S}imandl, M. and Dun\'{\i}k, J.},
  Journal                  = {Automatica},
  Year                     = {2009},
  Number                   = {7},
  Pages                    = {1749--1757},
  Volume                   = {45},

  Abstract                 = {The derivative-free nonlinear estimation methods exploiting the Stirlingâ€™s interpolation and the unscented transformation for discrete-time nonlinear stochastic systems are treated. The divided difference and unscented filters, smoothers, and predictors based on the methods are introduced in the unified framework. The new relations among the first order Stirlingâ€™s interpolation, the second order Stirlingâ€™s interpolation, and the unscented transformation are derived and their impact on the covariance matrices of the state estimates of the corresponding filters is analysed. The theoretical results are illustrated and used for the explanation of the unexpected behaviour of the sigma point Gaussian sum filters given as a mixture of the derivative-free filters.},
  Doi                      = {10.1016/j.automatica.2009.03.008},
  Keywords                 = {Estimation theory,Filtering techniques,Nonlinear systems,State estimation,Stochastic systems}
}

@Article{Simandl2006,
  Title                    = {{Advanced point-mass method for nonlinear state estimation}},
  Author                   = {\v{S}imandl, M. and Kr\'{a}lovec, J. and S\"{o}derstr\"{o}m, T.},
  Journal                  = {Automatica},
  Year                     = {2006},
  Number                   = {7},
  Pages                    = {1133--1145},
  Volume                   = {42},

  Doi                      = {10.1016/j.automatica.2006.03.010},
  Keywords                 = {nonlinear filters,point-mass method,probability density function,state estimation,stochastic systems}
}

@Article{Simandl2001,
  Title                    = {{Filtering, predictive, and smoothing Cram\'{e}r-Rao bounds for discrete-time nonlinear dynamic systems}},
  Author                   = {\v{S}imandl, M. and Kr\'{a}lovec, J. and Tichavsk\'{y}, P.},
  Journal                  = {Automatica},
  Year                     = {2001},
  Number                   = {11},
  Pages                    = {1703--1716},
  Volume                   = {37},

  Abstract                 = {Cram\'{e}râ€“Rao lower bounds for the discrete-time nonlinear state estimation problem are treated. The Cram\'{e}râ€“Rao bound for the mean-square error matrix of a state estimate is particularly important for quality evaluation of nonlinear state estimators as it represents a limit of cognizability of the state. Recursive relations for filtering, predictive, and smoothing Cram\'{e}râ€“Rao bounds are derived to establish a unifying framework for several previously published derivation procedures and results. Lower bounds for systems with unknown parameters are newly provided. Computation of filtering, predictive, and smoothing Cram\'{e}râ€“Rao bounds, their mutual comparison and utilization for quality evaluation of some nonlinear filters are shown in numerical examples.},
  Doi                      = {10.1016/S0005-1098(01)00136-4}
}

@Article{Azman2011,
  Title                    = {{Dynamical systems identification using Gaussian process models with incorporated local models}},
  Author                   = {A\v{z}man, K. and Kocijan, J.},
  Journal                  = {Engineering Applications of Artificial Intelligence},
  Year                     = {2011},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {398--408},
  Volume                   = {24},

  Doi                      = {10.1016/j.engappai.2010.10.010},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/A\v{z}man, Kocijan, Dynamical systems identification using Gaussian process models with incorporated local models, 2011.pdf:pdf},
  ISSN                     = {09521976},
  Keywords                 = {gaussian processes model,local models network,nonlinear system identification},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S095219761000196X}
}

@Book{Abramowitz1965,
  Title                    = {{Handbook of Mathematical Functions}},
  Author                   = {Abramowitz, M. and Stegun, I. A.},
  Publisher                = {Dover Publications},
  Year                     = {1965},

  ISBN                     = {978-0486612720}
}

@Book{Adler1981,
  Title                    = {Geometry of Random Fields},
  Author                   = {Adler, R. J.},
  Publisher                = {Wiley},
  Year                     = {1981},
  Pages                    = {302},

  ISBN                     = {978-0898716931}
}

@TechReport{Allen1958,
  Title                    = {{A Study of the Motion and Aerodynamic Heating of Ballistic Missiles Entering the Earth's Atmosphere at High Supersonic Speeds}},
  Author                   = {H. J. Allen and A. J. Eggers},
  Institution              = {NACA Ames Aeronautical Lab. Technical Report 1381},
  Year                     = {1958},
  Month                    = {January},

  Owner                    = {JPruher},
  Timestamp                = {2016.11.03}
}

@Article{Alspach1972,
  Title                    = {{Nonlinear Bayesian estimation using Gaussian sum approximations}},
  Author                   = {Alspach, D. and Sorenson, H.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {1972},
  Number                   = {4},
  Pages                    = {439--448},
  Volume                   = {17},

  Abstract                 = { Knowledge of the probability density function of the state conditioned on all available measurement data provides the most complete possible description of the state, and from this density any of the common types of estimates (e.g., minimum variance or maximum a posteriori) can be determined. Except in the linear Gaussian case, it is extremely difficult to determine this density function. In this paper an approximation that permits the explicit calculation of the a posteriori density from the Bayesian recursion relations is discussed and applied to the solution of the nonlinear filtering problem. In particular, it is noted that a weighted sum of Gaussian probability density functions can be used to approximate arbitrarily closely another density function. This representation provides the basis for procedure that is developed and discussed.},
  Doi                      = {10.1109/TAC.1972.1100034}
}

@Article{Andrieu2003,
  Title                    = {{An introduction to MCMC for machine learning}},
  Author                   = {Andrieu, C. and {De Freitas}, N. and Doucet, A. and Jordan, M. I.},
  Journal                  = {Machine Learning},
  Year                     = {2003},
  Pages                    = {5--43},
  Volume                   = {50},

  Abstract                 = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1109.4435v1},
  Doi                      = {10.1023/A:1020281327116},
  Eprint                   = {1109.4435v1},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Andrieu et al., An introduction to MCMC for machine learning, 2003.pdf:pdf},
  ISBN                     = {08856125 (ISSN)},
  ISSN                     = {0885-6125},
  Keywords                 = {0,1particle,Algorithms,Andrieu2003,AndrieuETAL.ML2003,AndrieuETAL03,BAYESIAN-ANALYSIS,Bayesian-Inference,Bioinformatics,Chain Monte-Carlo,Communication-Systems,Comprehensive Exam,Computation,Computer Science,Computer simulation,Data Augmentation,EM ALGORITHM,Folder - ISI,Folder - Search Algorithms,Folder - Teory\_not\_applied\_to\_video - MCMC,Folder - bayesian,Learning systems,Lit review,MCMC,MONTE-CARLO METHODS,Markov chain Monte Carlo,Markov chain Monte Carlo method,Markov processes,Markov-Chains,Metropolis Algorithms,Monte Carlo,Monte Carlo methods,NEURAL NETWORKS,Neural-Networks,Nonparametric processes,Posterior Distributions,Probabilistic Models in Cognition,Probabilistic machine learning,Probability,Sampling,Sampling Methods,State-Space,Stochastic algorithms,To read,UQ,Untagged,\_toread201305,baye,bayesian networks,carlo,chain,computational,dirichlet processes,file-import-08-12-28,file-import-10-02-11,gibbs,gibbs-sampling,graphical models,graphical-model,inPapers,introductory,learning,lista\_filtrada,machine learning,machine-learning,machine\_learning,machine\_learning monte\_carlo,machinelearning,markov,markov chain monte carlo,markov-chain,markov\_chain,markov\_chain\_monte\_carlo,markovchain,mc,mcmc,misc,ml,monte,montecarlo,p00504,p00565,p05287,p17778,particle filter,particle filtering,review,rjMCMC,sampling,statsnlp,stochastic,stochastic algorithms,topic-models,tutorial,tutorials,useful},
  Mendeley-tags            = {0,1particle,Andrieu2003,AndrieuETAL.ML2003,AndrieuETAL03,BAYESIAN-ANALYSIS,Bayesian-Inference,Bioinformatics,Chain Monte-Carlo,Communication-Systems,Comprehensive Exam,Computation,Computer Science,Data Augmentation,EM ALGORITHM,Folder - ISI,Folder - Search Algorithms,Folder - Teory\_not\_applied\_to\_video - MCMC,Folder - bayesian,Lit review,MONTE-CARLO METHODS,Markov-Chains,Metropolis Algorithms,Monte Carlo,NEURAL NETWORKS,Neural-Networks,Nonparametric processes,Posterior Distributions,Probabilistic Models in Cognition,Sampling Methods,State-Space,To read,UQ,Untagged,\_toread201305,baye,bayesian networks,carlo,chain,computational,dirichlet processes,file-import-08-12-28,file-import-10-02-11,gibbs,gibbs-sampling,graphical models,graphical-model,inPapers,introductory,learning,lista\_filtrada,machine learning,machine-learning,machine\_learning,machine\_learning monte\_carlo,machinelearning,markov,markov chain monte carlo,markov-chain,markov\_chain,markov\_chain\_monte\_carlo,markovchain,mc,mcmc,misc,ml,monte,montecarlo,p00504,p00565,p05287,p17778,particle filter,particle filtering,review,rjMCMC,sampling,statsnlp,stochastic,stochastic algorithms,topic-models,tutorial,tutorials,useful},
  Pmid                     = {178037200001}
}

@Article{Arasaratnam2009,
  Title                    = {{Cubature Kalman Filters}},
  Author                   = {Arasaratnam, I. and Haykin, S.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2009},
  Number                   = {6},
  Pages                    = {1254--1269},
  Volume                   = {54}
}

@Article{Arulampalam2002,
  Title                    = {{A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking}},
  Author                   = {Arulampalam, M. S. and Maskell, S. and Gordon, N. and Clapp, T.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {174--188},
  Volume                   = {50},

  Doi                      = {10.1109/78.978374}
}

@Article{Athans1968,
  Title                    = {{Suboptimal State Estimation for Continuous-Time Nonlinear Systems from Discrete Noisy Measurements}},
  Author                   = {Athans, M. and Wishner, R. and Bertolini, A.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {1968},
  Number                   = {5},
  Pages                    = {504--514},
  Volume                   = {13},

  Abstract                 = {This paper presents the derivation of the dynamical equations of a second-order filter which estimates the states of a non-linear plant on the basis of discrete noisy measurements. The filter equations contain terms involving the second-order partial derivatives of the plant and output equations. Simulation results are presented which yield a comparison of the performance of the first-versus the second-order filter when applied to a nonlinear third-order system. The results indicate that the inclusion of second-order terms can markedly improve the filter performance.},
  Doi                      = {10.1109/TAC.1968.1098986}
}

@Article{Bach2015,
  Title                    = {{On the Equivalence between Quadrature Rules and Random Features}},
  Author                   = {Bach, F.},
  Year                     = {2015},
  Pages                    = {1--25},

  Abstract                 = {We showthat kernel-based quadrature rules for computing integrals are a special case
of random feature expansions for positive definite kernels for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a known non- uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in L2- and L\infinity-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1502.06800v1},
  Eprint                   = {arXiv:1502.06800v1}
}

@Book{Barber2012,
  Title                    = {{Bayesian Reasoning and Machine Learning}},
  Author                   = {Barber, D.},
  Publisher                = {Cambridge University Press},
  Year                     = {2012},
  Pages                    = {697},

  ISBN                     = {978-0-521-51814-7}
}

@InProceedings{Barber2007,
  Title                    = {{Unified Inference for Variational Bayesian Linear Gaussian State-Space Model}},
  Author                   = {Barber, D. and Chiappa, S.},
  Booktitle                = {Advances in Neural Information Processing Systems 19},
  Year                     = {2007},
  Editor                   = {Sch\"{o}lkopf, B. and Platt, J.C. and Hoffman, T.},
  Pages                    = {81--88},
  Publisher                = {MIT Press},

  Abstract                 = {Linear Gaussian State-Space Models are widely used and a Bayesian treatment of parameters is therefore of considerable interest. The approximate Variational Bayesian method applied to these models is an attractive approach, used successfully in applications ranging from acoustics to bioinformatics. The most challenging aspect of implementing the method is in performing inference on the hidden state sequence of the model. We show how to convert the inference problem so that standard and stable Kalman Filtering/Smoothing recursions from the literature may be applied. This is in contrast to previously published approaches based on Belief Propagation. Our framework both simplifies and unifies the inference problem, so that future applications may be easily developed. We demonstrate the elegance of the approach on Bayesian temporal ICA, with an application to finding independent components in noisy EEG signals.},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Barber, Chiappa, Unified Inference for Variational Bayesian Linear Gaussian State-Space Model, 2007.pdf:pdf},
  ISBN                     = {9780262195683},
  ISSN                     = {1049-5258}
}

@Article{Barillec2011b,
  Title                    = {{Projected sequential Gaussian processes: A C++ tool for interpolation of large datasets with heterogeneous noise}},
  Author                   = {Barillec, R. and Ingram, B. and Cornford, D. and Csat\'{o}, L.},
  Journal                  = {Computers \& Geosciences},
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {295--309},
  Volume                   = {37},

  Doi                      = {10.1016/j.cageo.2010.05.008},
  Keywords                 = {low-rank approximations}
}

@Book{Bar-Shalom2001,
  Title                    = {{Estimation with Applications to Tracking and Navigation}},
  Author                   = {Bar-Shalom, Y. and Li, X. R. and Kirubarajan, T.},
  Publisher                = {Wiley-Blackwell},
  Year                     = {2001},
  Pages                    = {584},

  Annote                   = {Interacting Multiple Model estimator (IMM)},
  ISBN                     = {978-0471416555}
}

@Article{Bell1993,
  Title                    = {{Iterated Kalman filter update as a Gauss-Newton method}},
  Author                   = {Bell, Bradley M. and Cathey, Frederick W.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {294--297},
  Volume                   = {38},

  Abstract                 = {It is shown that the iterated Kalman filter (IKF) update is an
application of the Gauss-Newton method for approximating a maximum
likelihood estimate. An example is presented in which the iterated
Kalman filter update and maximum likelihood estimate show correct
convergence behavior as the observation becomes more accurate, whereas
the extended Kalman filter update does not},
  Doi                      = {10.1109/9.250476},
  Pmid                     = {250476}
}

@Article{Bellantoni1967,
  Title                    = {{A square root formulation of the Kalman- Schmidt filter}},
  Author                   = {Bellantoni, J. F. and Dodge, K. W.},
  Journal                  = {AIAA Journal},
  Year                     = {1967},
  Number                   = {7},
  Pages                    = {1309--1314},
  Volume                   = {5},

  Annote                   = {Introduction of EKF (Kalman-Schmidt filter)},
  Doi                      = {10.2514/3.4189},
  Keywords                 = {EKF},
  Language                 = {en},
  Mendeley-tags            = {EKF}
}

@Book{Bhar2010,
  Title                    = {{Stochastic filtering with applications in finance}},
  Author                   = {Bhar, R.},
  Publisher                = {World Scientific},
  Year                     = {2010},

  ISBN                     = {978-981-4304-85-6},
  Mendeley-groups          = {Books}
}

@Book{Bishop2007,
  Title                    = {{Pattern Recognition and Machine Learning}},
  Author                   = {Bishop, C. M.},
  Publisher                = {Springer},
  Year                     = {2007},
  Pages                    = {738},

  ISBN                     = {978-0387310732}
}

@Book{Boyd2004,
  Title                    = {{Convex Optimization}},
  Author                   = {Boyd, S. and Vandenberghe, L.},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},
  Pages                    = {727},

  ISBN                     = {978-0-521-83378-3}
}

@InCollection{Boyle2005,
  Title                    = {{Dependent Gaussian Processes}},
  Author                   = {Boyle, P. and Frean, M.},
  Booktitle                = {Advances in Neural Information Processing Systems 17},
  Publisher                = {MIT Press},
  Year                     = {2005},
  Editor                   = {Saul, L. K. and Weiss, Y. and Bottou, L.},
  Pages                    = {217--224},

  Abstract                 = {Gaussian processes are usually parameterised in terms of their covariance functions. However, this makes it difficult to deal with multiple outputs, because ensuring that the covariance matrix is positive definite is problematic. An alternative formulation is to treat Gaussian processes as white noise sources convolved with smoothing kernels, and to parameterise the kernel instead. Using this, we extend Gaussian processes to handle multiple, coupled outputs.},
  Keywords                 = {gaussian processes,multiple outputs}
}

@Article{Briol2015,
  Title                    = {{Probabilistic Integration: A Role for Statisticians in Numerical Analysis?}},
  Author                   = {{Briol}, F.-X. and {Oates}, C.~J. and {Girolami}, M. and {Osborne}, M.~A. and {Sejdinovic}, D.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2015},

  Month                    = dec,

  Abstract                 = {Probabilistic numerical methods aim to model numerical error as a source of epistemic uncertainty that is subject to probabilistic analysis and reasoning, enabling the principled propagation of numerical uncertainty through a computational pipeline. In this paper we focus on numerical methods for integration. We present probabilistic (Bayesian) versions of both Markov chain and Quasi Monte Carlo methods for integration and provide rigorous theoretical guarantees for convergence rates, in both posterior mean and posterior contraction. The performance of probabilistic integrators is guaranteed to be no worse than non-probabilistic integrators and is, in many cases, asymptotically superior. These probabilistic integrators therefore enjoy the "best of both worlds", leveraging the sampling efficiency of advanced Monte Carlo methods whilst being equipped with valid probabilistic models for uncertainty quantification. Several applications and illustrations are provided, including examples from computer vision and system modelling using non-linear differential equations. A survey of open challenges in probabilistic integration is provided.},
  Archiveprefix            = {arXiv},
  Eprint                   = {1512.00933},
  Keywords                 = {Statistics - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Statistics Theory, Statistics - Computation},
  Primaryclass             = {stat.ML}
}

@Article{Bucy1971,
  Title                    = {{Digital Synthesis of Non-linear Filters}},
  Author                   = {Bucy, R. S. and Senne, K. D.},
  Journal                  = {Automatica},
  Year                     = {1971},
  Pages                    = {287--298},
  Volume                   = {7}
}

@InCollection{Bui2014,
  Title                    = {{Tree-structured Gaussian Process Approximations}},
  Author                   = {Bui, T. D. and Turner, R. E.},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2014},
  Editor                   = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N.D. and Weinberger, K.Q.},
  Pages                    = {2213--2221},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Bui, Turner, Tree-structured Gaussian Process Approximations, 2014.pdf:pdf},
  Mendeley-groups          = {Gaussian Processes/Approximations}
}

@InProceedings{Carli2012,
  Title                    = {{On the estimation of hyperparameters for Bayesian system identification with exponentially decaying kernels}},
  Author                   = {Carli, F. and Chen, T. and Chiuso, A. and Ljung, L. and Pillonetto, G.},
  Booktitle                = {Decision and Control (CDC), 2012 IEEE 51st Annual Conference on},
  Year                     = {2012},
  Month                    = dec,
  Pages                    = {5260--5265},
  Publisher                = {Ieee},

  Doi                      = {10.1109/CDC.2012.6426236},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Carli et al., On the estimation of hyperparameters for Bayesian system identification with exponentially decaying kernels, 2012.pdf:pdf},
  ISBN                     = {978-1-4673-2066-5},
  Url                      = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6426236}
}

@Article{Chalupka2013a,
  Title                    = {{A Framework for Evaluating Approximation Methods for Gaussian Process Regression}},
  Author                   = {Chalupka, K. and Williams, C. K. I. and Murray, I.},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2013},
  Pages                    = {333--350},
  Volume                   = {14},

  Keywords                 = {fitc,gaussian process regression,local gp,subset of data}
}

@InProceedings{Chen2007,
  Title                    = {{Nonparametric Identification of Affine Nonlinear Systems}},
  Author                   = {Chen, X.-M. and Gao, C. and Wang, L.},
  Booktitle                = {Proceedings of the 32nd Chinese Control Conference},
  Year                     = {2007},
  Number                   = {61203118},
  Pages                    = {2007--2011},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Chen, Gao, Wang, Nonparametric Identification of Affine Nonlinear Systems, 2007.pdf:pdf},
  Keywords                 = {affine nonlinear system,markov chain,nonparametric kernel regression estimation,recursive identification}
}

@Article{Chen2003,
  Title                    = {{Bayesian Filtering : From Kalman Filters to Particle Filters, and Beyond}},
  Author                   = {Chen, Z.},
  Journal                  = {Statistics},
  Year                     = {2003},
  Number                   = {1},
  Pages                    = {1--69},
  Volume                   = {182},

  Keywords                 = {Adaptive Systems Lab,McMaster University}
}

@Article{Chowdhary2014,
  Title                    = {{Bayesian Nonparametric Adaptive Control Using Gaussian Processes}},
  Author                   = {Chowdhary, G. and Kingravi, H. A. and How, J. P. and Vela, P. A.},
  Journal                  = {Neural Networks and Learning Systems, IEEE Transactions on},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {537--550},
  Volume                   = {26},

  Doi                      = {10.1109/TNNLS.2014.2319052}
}

@InProceedings{Chowdhary2013,
  Title                    = {{A Bayesian nonparametric approach to adaptive control using Gaussian Processes}},
  Author                   = {Chowdhary, G. and Kingravi, H. A. and How, J. P. and Vela, P. A.},
  Booktitle                = {IEEE Conference on Decision and Control},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {874--879},
  Publisher                = {IEEE},
  Volume                   = {2},

  Doi                      = {10.1109/CDC.2013.6759992}
}

@Book{Cressie1993,
  Title                    = {{Statistics for Spatial Data}},
  Author                   = {Cressie, N. A. C.},
  Publisher                = {Wiley},
  Year                     = {1993},
  Pages                    = {928},

  ISBN                     = {978-0471002550}
}

@InProceedings{Cruz2012a,
  Title                    = {{Online Learning of Inverse Dynamics via Gaussian Process Regression}},
  Author                   = {Cruz, J. S. De La and Owen, W. and Kuli\'{c}, D.},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {3583--3590}
}

@Article{Csato2002,
  Title                    = {{Sparse Online Gaussian Processes}},
  Author                   = {Csat\'{o}, L. and Opper, M.},
  Journal                  = {Neural Computation},
  Year                     = {2002},
  Number                   = {3},
  Pages                    = {641--668},
  Volume                   = {14},

  Annote                   = {Sparse Online GP regression, without recursive hyperparameter learning (see p. 13).},
  Publisher                = {MIT Press}
}

@Article{Dallaire2009a,
  Title                    = {{Learning Gaussian Process Models from Uncertain Data}},
  Author                   = {Dallaire, P. and Besse, C. and Chaib-Draa, B.},
  Journal                  = {Neural Information Processing},
  Year                     = {2009},

  Keywords                 = {dynamical systems,gaussian processes,noisy inputs}
}

@InCollection{Damianou2011,
  Title                    = {{Variational Gaussian Process Dynamical Systems}},
  Author                   = {Damianou, A. and Titsias, M. K. and Lawrence, N. D.},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2011},
  Editor                   = {Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F and Weinberger, K Q},
  Pages                    = {2510--2518}
}

@Book{DasGupta2011,
  Title                    = {{Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics}},
  Author                   = {DasGupta, A.},
  Publisher                = {Springer Science \& Business Media},
  Year                     = {2011},
  Pages                    = {782},

  ISBN                     = {978-1-4419-9633-6}
}

@Article{Daum2003,
  Title                    = {{Curse of Dimensionality and Particle Filters}},
  Author                   = {Daum, F. and Huang, J.},
  Journal                  = {Proceedings of the IEEE Aerospace Conference},
  Year                     = {2003},
  Pages                    = {1979--1993},
  Volume                   = {4}
}

@InCollection{Deisenroth2012a,
  Title                    = {Expectation Propagation in Gaussian Process Dynamical Systems},
  Author                   = {Deisenroth, M. and Mohamed, S.},
  Booktitle                = {Advances in Neural Information Processing Systems 25},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2012},
  Editor                   = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
  Pages                    = {2609--2617},

  __markedentry            = {[JPruher:]}
}

@PhdThesis{Deisenroth2009b,
  Title                    = {{Efficient Reinforcement Learning Using Gaussian Processes}},
  Author                   = {Deisenroth, M. P.},
  School                   = {KIT},
  Year                     = {2009},

  ISBN                     = {9783866445697}
}

@InProceedings{Deisenroth2009,
  Title                    = {{Analytic moment-based Gaussian process filtering}},
  Author                   = {Deisenroth, M. P. and Huber, M. F. and Hanebeck, U. D.},
  Booktitle                = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
  Year                     = {2009},
  Pages                    = {1--8},
  Publisher                = {ACM Press},

  Annote                   = {Proposes GP-ADF filter},
  Doi                      = {10.1145/1553374.1553403}
}

@InProceedings{Deisenroth2011,
  Title                    = {{A General Perspective on Gaussian Filtering and Smoothing: Explaining Current and Deriving New Algorithms}},
  Author                   = {Deisenroth, M. P. and Ohlsson, H.},
  Booktitle                = {Proceedings of the 2011 American Control Conference},
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {1807--1812},
  Publisher                = {IEEE},
  Volume                   = {1},

  Doi                      = {10.1109/ACC.2011.5990871}
}

@Article{Deisenroth2011a,
  Title                    = {{PILCO: A model-based and data-efficient approach to policy search}},
  Author                   = {Deisenroth, M. P. and Rasmussen, C. E.},
  Journal                  = {Proceedings of the \ldots},
  Year                     = {2011}
}

@Article{Deisenroth2012,
  Title                    = {{Robust Filtering and Smoothing with Gaussian Processes}},
  Author                   = {Deisenroth, M. P. and Turner, R. D. and Huber, M. F. and Hanebeck, U. D. and Rasmussen, C. E.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2012},
  Number                   = {7},
  Pages                    = {1865--1871},
  Volume                   = {57},

  Doi                      = {10.1109/TAC.2011.2179426},
  Mendeley-tags            = {filtering,gaussian processes,smoothing},
  Shorttitle               = {Automatic Control, IEEE Transactions on}
}

@Article{DelMoral1996,
  Title                    = {{Nonlinear Filtering: Interacting Particle Resolution}},
  Author                   = {{Del Moral}, P.},
  Journal                  = {Markov processes and Related Fields},
  Year                     = {1996},
  Number                   = {4},
  Pages                    = {555--581},
  Volume                   = {2}
}

@Article{Dempster1977,
  Title                    = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
  Author                   = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  Journal                  = {Journal of the Royal Statistical Society},
  Year                     = {1977},
  Pages                    = {1----38},
  Volume                   = {39},

  Annote                   = {Introduces EM algorithm},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Dempster, Laird, Rubin, Maximum Likelihood from Incomplete Data via the EM Algorithm, 1977.pdf:pdf}
}

@InProceedings{Diaconis1988,
  Title                    = {{Bayesian numerical analysis}},
  Author                   = {Diaconis, P.},
  Booktitle                = {Statistical Decision Theory and Related Topics IV},
  Year                     = {1988},
  Editor                   = {Gupta, S. S. and Berger, J. O.},
  Pages                    = {163--175},
  Publisher                = {Springer}
}

@Article{Doucet2001,
  Title                    = {{Sequential Monte Carlo Methods in Practice}},
  Author                   = {Doucet, A. and {De Freitas}, N. and Gordon, N.},
  Journal                  = {Springer New York},
  Year                     = {2001},
  Pages                    = {178--195},

  Doi                      = {10.1198/tech.2003.s23},
  ISBN                     = {0387951466},
  ISSN                     = {1530-888X},
  Pmid                     = {10770839}
}

@Article{Doucet2000,
  Title                    = {On sequential Monte Carlo sampling methods for Bayesian filtering},
  Author                   = {Doucet, A. and Godsill, S. and Andrieu, C.},
  Journal                  = {Statistics and Computing},
  Year                     = {2000},
  Pages                    = {197-208},
  Volume                   = {10},

  Abstract                 = {In this article, we present an overview of methods for sequential simulation from posterior distribu-
tions. These methods are of particular interest in Bayesian filtering for discrete time dynamic models
that are typically nonlinear and non-Gaussian. A general importance sampling framework is devel-
oped that unifies many of the methods which have been proposed over the last few decades in several
different scientific disciplines. Novel extensions to the existing methods are also proposed. We show in
particular how to incorporate local linearisation methods similar to those which have previously been
employed in the deterministic filtering literature; these lead to very effective importance distributions.
Furthermore we describe a method which uses Rao-Blackwellisation in order to take advantage of
the analytic structure present in some important classes of state-space models. In a final section we
develop algorithms for prediction, smoothing and evaluation of the likelihood in dynamic models.}
}

@Article{Dunik2013,
  Title                    = {{Stochastic Integration Filter}},
  Author                   = {Dun\'{\i}k, J. and Straka, O. and \v{S}imandl, M.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {1561--1566},
  Volume                   = {58}
}

@PhdThesis{Duvenaud2014,
  Title                    = {{Automatic Model Construction with Gaussian Processes}},
  Author                   = {Duvenaud, D. K.},
  School                   = {University of Cambridge},
  Year                     = {2014}
}

@InProceedings{Frigola2014a,
  Title                    = {{Variational Gaussian Process State-Space Models}},
  Author                   = {Frigola, R. and Chen, Y. and Rasmussen, C. E.},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Year                     = {2014},
  Editor                   = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  Pages                    = {3680--3688},
  Publisher                = {Curran Associates, Inc.},

  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1406.4905v1},
  Eprint                   = {arXiv:1406.4905v1}
}

@InProceedings{Frigola2013,
  Title                    = {{Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC}},
  Author                   = {Frigola, R. and Lindsten, F.},
  Booktitle                = {Neural Information Processing Systems},
  Year                     = {2013},
  Pages                    = {1--9},

  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1306.2861v2},
  Eprint                   = {arXiv:1306.2861v2}
}

@Article{Frigola2013a,
  Title                    = {{Identification of Gaussian Process State-Space Models with Particle Stochastic Approximation EM}},
  Author                   = {Frigola, R. and Lindsten, F. and Sch\"{o}n, T. B. and Rasmussen, C. E.},
  Journal                  = {arXiv preprint arXiv: \ldots},
  Year                     = {2013},

  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1312.4852v1},
  Eprint                   = {arXiv:1312.4852v1},
  Keywords                 = {bayesian,gaussian processes,non-parametric identification,system identification}
}

@Article{Frigolab,
  Title                    = {{Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes}},
  Author                   = {Frigola, R. and Rasmussen, C. E.},
  Journal                  = {arxiv-web1.library.cornell.edu},
  Number                   = {3},

  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1303.2912v3},
  Eprint                   = {arXiv:1303.2912v3},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Frigola, Rasmussen, Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes, Unknown.pdf:pdf},
  Url                      = {http://arxiv-web1.library.cornell.edu/pdf/1303.2912.pdf}
}

@Article{Friston2010,
  Title                    = {{Generalised filtering}},
  Author                   = {Friston, K. and Stephan, K. and Li, B. and Daunizeau, J.},
  Journal                  = {Mathematical Problems in Engineering},
  Year                     = {2010},
  Pages                    = {34},
  Volume                   = {2010},

  Abstract                 = {We describe a Bayesian filtering scheme for nonlinear state-space models in continuous time. This scheme is called Generalised Filtering and furnishes posterior (conditional) densities on hidden states and unknown parameters generating observed data. Crucially, the scheme operates online, assimilating data to optimize the conditional density on time-varying states and time-invariant parameters. In contrast to Kalman and Particle smoothing, Generalised Filtering does not require a backwards pass. In contrast to variational schemes, it does not assume conditional independence between the states and parameters. Generalised Filtering optimises the conditional density with respect to a free-energy bound on the model's log-evidence. This optimisation uses the generalised motion of hidden states and parameters, under the prior assumption that the motion of the parameters is small. We describe the scheme, present comparative evaluations with a fixed-form variational version, and conclude with an illustrative application to a nonlinear state-space model of brain imaging time-series.},
  Doi                      = {10.1155/2010/621670}
}

@Article{Fritsche,
  Title                    = {{A Fresh Look at Bayesian Cram\'{e}r-Rao Bounds for Discrete-Time Nonlinear Filtering}},
  Author                   = {Fritsche, C. and \"{O}zkan, E. and Svensson, L. and Gustafsson, F.}
}

@InProceedings{Garnett2014,
  Title                    = {{Active Learning of Linear Embeddings for Gaussian Processes}},
  Author                   = {Garnett, R. and Osborne, M. A. and Hennig, P.},
  Booktitle                = {Uncertainty in Artificial Intellignece (UAI 2014), 30th Conference on},
  Year                     = {2014},

  Abstract                 = {We propose an active learning method for discovering low-dimensional structure in high- dimensional Gaussian process (GP) tasks. Such problems are increasingly frequent and impor- tant, but have hitherto presented severe practical difficulties. We further introduce a novel tech- nique for approximately marginalizing GP hyper- parameters, yielding marginal predictions robust to hyperparameter misspecification. Our method offers an efficient means of performing GP re- gression, quadrature, or Bayesian optimization in high-dimensional spaces.},
  Annote                   = {Describes the approximate GP hyperparameter marginalization in BQ.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1310.6740v1},
  Eprint                   = {arXiv:1310.6740v1},
  Url                      = {http://arxiv.org/abs/1310.6740}
}

@Book{Gautschi2004,
  Title                    = {Orthogonal Polynomials: Computation and Approximation},
  Author                   = {Gautschi, W.},
  Publisher                = {Oxford University Press},
  Year                     = {2004},
  Pages                    = {312},
  Series                   = {Numerical Mathematics and Scientific Computation},

  ISBN                     = {978-0198506720}
}

@Book{Gelb1974,
  Title                    = {{Applied Optimal Estimation}},
  Author                   = {Gelb, A.},
  Publisher                = {The MIT Press},
  Year                     = {1974},
  Pages                    = {384},

  ISBN                     = {978-0262570480}
}

@Book{Gelman2013,
  Title                    = {{Bayesian Data Analysis}},
  Author                   = {Gelman, A.},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {2013},

  Edition                  = {3rd},
  Pages                    = {675},

  ISBN                     = {978-1439840955}
}

@Article{Genz1999,
  Title                    = {{A stochastic algorithm for high-dimensional integrals over unbounded regions with Gaussian weight}},
  Author                   = {Genz, A. and Monahan, J.},
  Journal                  = {Journal of Computational and Applied Mathematics},
  Year                     = {1999},
  Number                   = {1-2},
  Pages                    = {71--81},
  Volume                   = {112},

  Doi                      = {10.1016/S0377-0427(99)00214-9},
  Keywords                 = {gaussian weight,high-dimensional integral,monte carlo}
}

@Article{Genz1996,
  Title                    = {{Stochastic Integration Rules for Infinite Regions}},
  Author                   = {Genz, A. and Monahan, J.},
  Journal                  = {SIAM Journal on Scientific Computation},
  Year                     = {1996},
  Pages                    = {426----439},
  Volume                   = {19},

  Doi                      = {10.1007/978-1-4757-9727-5\_1}
}

@Article{Gerstner98,
  Title                    = {Numerical integration using sparse grids},
  Author                   = {Gerstner, Thomas and Griebel, Michael},
  Journal                  = {Numerical Algorithms},
  Year                     = {1998},
  Number                   = {3-4},
  Pages                    = {209-232},
  Volume                   = {18},

  Doi                      = {10.1023/A:1019129717644},
  Keywords                 = {multivariate numerical quadrature; Smolyak’s construction; sparse grids; complexity; curse of dimension; 65C20; 65D30; 65D32; 65M99; 65R20; 65U05; 65Y20},
  Publisher                = {Kluwer Academic Publishers}
}

@Article{Ghahramani2013,
  Title                    = {{Bayesian nonparametrics and the probabilistic approach to modelling}},
  Author                   = {Ghahramani, Z.},
  Journal                  = {Philosophical Transactions of the Royal Society A},
  Year                     = {2013},
  Number                   = {1984},
  Pages                    = {1--27},
  Volume                   = {371},

  Doi                      = {10.1098/rspa.00000000},
  ISSN                     = {1364-503X},
  Keywords                 = {bayesian statistics,machine learning,nonparametrics,probabilistic modelling},
  Pmid                     = {23277609}
}

@InProceedings{Gillijns2006,
  Title                    = {What is the ensemble Kalman filter and how well does it work?},
  Author                   = {Gillijns, S. and Mendoza, O.B. and Chandrasekar, J. and De Moor, B.L.R. and Bernstein, D.S. and Ridley, A.},
  Booktitle                = {American Control Conference, 2006},
  Year                     = {2006},
  Month                    = {June},
  Pages                    = {6},

  Doi                      = {10.1109/ACC.2006.1657419}
}

@InProceedings{Girard2003,
  Title                    = {{Gaussian Process Priors With Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting}},
  Author                   = {Girard, A. and Rasmussen, C. E. and Qui\~{n}onero-Candela, J. and Murray-Smith, R.},
  Booktitle                = {Advances in Neural Information Processing Systems 15},
  Year                     = {2003},
  Editor                   = {Becker, S. and Thrun, S. and Obermayer, K.},
  Pages                    = {545--552},
  Publisher                = {MIT Press}
}

@Book{Golub2012,
  Title                    = {{Matrix Computations}},
  Author                   = {Golub, G. H. and {Van Loan}, C. F.},
  Publisher                = {Johns Hopkins University Press},
  Year                     = {2012},
  Pages                    = {784},

  ISBN                     = {978-1421407944}
}

@Article{Gordon1993,
  Title                    = {{Novel approach to nonlinear/non-Gaussian Bayesian state estimation}},
  Author                   = {Gordon, N. J. and Salmond, D. J. and Smith, A. F. M.},
  Journal                  = {IEE Proceedings F (Radar and Signal Processing)},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {107--113},
  Volume                   = {140},

  Annote                   = {Bootstrap filter, introduction of resampling step made the SMC approach applicable (further developed into PF)},
  Publisher                = {IET}
}

@Article{Gorodetsky2016,
  Title                    = {{Mercer kernels and integrated variance experimental design: connections between Gaussian process regression and polynomial approximation}},
  Author                   = {Gorodetsky, A. and Marzouk, Y.},
  Journal                  = {SIAM/ASA Journal on Uncertainty Quantification},
  Year                     = {2016},
  Pages                    = {796--828},
  Volume                   = {4},

  Abstract                 = {This paper examines experimental design procedures used to develop surrogates of computational models, exploring the interplay between experimental designs and approximation algorithms. We focus on two widely used approximation approaches, Gaussian process (GP) regression and non-intrusive polynomial approximation. First, we introduce algorithms for minimizing a posterior integrated variance (IVAR) design criterion for GP regression. Our formulation treats design as a continuous optimization problem that can be solved with gradient-based methods on complex input domains, without resorting to greedy approximations. We show that minimizing IVAR in this way yields point sets with good interpolation properties, and that it enables more accurate GP regression than designs based on entropy minimization or mutual information maximization. Second, using a Mercer kernel/eigenfunction perspective on GP regression, we identify conditions under which GP regression coincides with pseudospectral polynomial approximation. Departures from these conditions can be understood as changes either to the kernel or to the experimental design itself. We then show how IVAR-optimal designs, while sacrificing discrete orthogonality of the kernel eigenfunctions, can yield lower approximation error than orthogonalizing point sets. Finally, we compare the performance of adaptive Gaussian process regression and adaptive pseudospectral approximation for several classes of target functions, identifying features that are favorable to the GP + IVAR approach.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1503.00021},
  Eprint                   = {1503.00021},
  Keywords                 = {approximation theory,computer experiments,experimental design,gaussian process regression,kernel interpolation,polynomial approximation,uncertainty quantification},
  Url                      = {http://arxiv.org/abs/1503.00021}
}

@InProceedings{Grande2013,
  Title                    = {{Nonparametric Adaptive Control using Gaussian Processes with Online Hyperparameter Estimation}},
  Author                   = {Grande, R. C. and Chowdhary, G. and How, J. P.},
  Booktitle                = {IEEE Conference on Decision and Control},
  Year                     = {2013},
  Pages                    = {861--867},
  Publisher                = {Ieee},

  Doi                      = {10.1109/CDC.2013.6759990}
}

@Book{Grewal2007,
  Title                    = {{Global Positioning Systems, Inertial Navigation, and Integration}},
  Author                   = {Grewal, M. S. and Weill, L. R. and Andrews, A. P.},
  Publisher                = {Wiley},
  Year                     = {2007},
  Pages                    = {416},

  ISBN                     = {978-0-470-09971-1},
  Mendeley-groups          = {Books}
}

@Book{Grigoriu2002,
  Title                    = {{Stochastic Calculus: Applications in Science and Engineering}},
  Author                   = {Grigoriu, M.},
  Publisher                = {Birkh\"{a}user},
  Year                     = {2002},
  Pages                    = {792},

  ISBN                     = {978-0817642426}
}

@Book{Grimmett2001,
  Title                    = {{Probability and Random Processes}},
  Author                   = {Grimmett, G. and Stirzaker, D.},
  Publisher                = {Oxford University Press},
  Year                     = {2001},

  Edition                  = {3rd},
  Pages                    = {608},

  ISBN                     = {978-0198572220}
}

@InCollection{Gunter2014,
  Title                    = {{Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature}},
  Author                   = {Gunter, T. and Osborne, M. A. and Garnett, R. and Hennig, P. and Roberts, S. J.},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2014},
  Editor                   = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  Pages                    = {2789--2797}
}

@Article{Guo2006,
  Title                    = {Quasi-Monte Carlo filtering in nonlinear dynamic systems},
  Author                   = {Dong Guo and Xiaodong Wang},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {2006},
  Number                   = {6},
  Pages                    = {2087-2098},
  Volume                   = {54},

  Doi                      = {10.1109/TSP.2006.873585},
  Keywords                 = {Bayes methods;Kalman filters;filtering theory;importance sampling;nonlinear filters;Bayesian filtering;adaptive importance sampling;deterministic filtering;extended Kalman filter;nonlinear dynamic systems;quadrature Kalman filter;quasi-Monte Carlo filtering;unscented Kalman filter;Adaptive filters;Bayesian methods;Filtering algorithms;Integral equations;Jacobian matrices;Monte Carlo methods;Nonlinear dynamical systems;Nonlinear equations;Signal processing algorithms;Sliding mode control;Kalman filter (KF);Nonlinear dynamic systems;quasi-Monte Carlo (QMC);sequential Monte Carlo (SMC)}
}

@Article{Gustafsson2010,
  Title                    = {{Praticle Filter Theory and Practice with Positioning Applications}},
  Author                   = {Gustafsson, F.},
  Journal                  = {IEEE Aerospace and Electronic Systems Magazine},
  Year                     = {2010},
  Number                   = {7},
  Pages                    = {53--81},
  Volume                   = {25}
}

@Article{Gustafsson2012,
  Title                    = {{Some Relations Between Extended and Unscented Kalman Filters}},
  Author                   = {Gustafsson, F. and Hendeby, G.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {545--555},
  Volume                   = {60}
}

@Article{Hall2012a,
  Title                    = {{Modelling and control of nonlinear systems using Gaussian processes with partial model information}},
  Author                   = {Hall, J. and Rasmussen, C. E. and Maciejowski, J.},
  Journal                  = {2012 IEEE 51st IEEE Conference on Decision and Control (CDC)},
  Year                     = {2012},
  Pages                    = {5266--5271},

  Doi                      = {10.1109/CDC.2012.6426746},
  Publisher                = {Ieee}
}

@Article{Handschin1969,
  Title                    = {{Monte Carlo techniques to estimate the conditional expectation in multi-stage non-linear filtering}},
  Author                   = {Handschin, J. E. and Mayne, D. Q.},
  Journal                  = {International Journal of Control},
  Year                     = {1969},
  Number                   = {5},
  Pages                    = {547--559},
  Volume                   = {9},

  Abstract                 = {Using Bayes' theorem the conditional mean of the posterior probability density function is estimated via Monte Carlo techniques. Multi-stage, non-linear filtering requires the solution of high dimensional integrals. The new feature of the approach presented is that a combination of analytical and numerical methods yields a variance reduction which can also be interpreted as an accuracy improvement of approximate non-linear filter equations. Theorems are derived to prove zero sampling variance for the linear Gaussian case and experimental results indicate that the proposed estimators are feasible in non-linear situations.},
  Annote                   = {Monte Carlo Intro},
  Doi                      = {10.1080/00207176908905777},
  Keywords                 = {SMC},
  Mendeley-tags            = {SMC}
}

@Article{Hartikainen2010b,
  Title                    = {{Kalman filtering and smoothing solutions to temporal Gaussian process regression models}},
  Author                   = {Hartikainen, J. and S\"{a}rkk\"{a}, S.},
  Journal                  = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
  Year                     = {2010},
  Pages                    = {379--384},

  Annote                   = {Computing GP regression solution in O(T) (instead of O(T\^{}3)) by converting the covarinace function description of the GP into state-space representation (SDE representation) and perfoming Kalman filtering and smoothing.}
}

@Book{Hastie2009,
  Title                    = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
  Author                   = {Hastie, T. and Tibshirani, R. and Friedman, J.},
  Publisher                = {Springer},
  Year                     = {2009},

  Edition                  = {2nd},
  Pages                    = {745},

  ISBN                     = {978-0387848570}
}

@Book{Haug2012,
  Title                    = {{Bayesian Estimation and Tracking: A Practical Guide}},
  Author                   = {Haug, A. J.},
  Publisher                = {Wiley-Blackwell},
  Year                     = {2012},
  Pages                    = {400},

  ISBN                     = {978-0470621707}
}

@InProceedings{Heine2006,
  Title                    = {{Robust Model Predictive Control using the Unscented Transformation}},
  Author                   = {T. Heine and M. Kawohl and R. King},
  Booktitle                = {2006 IEEE Conference on Computer Aided Control System Design},
  Year                     = {2006},
  Month                    = {Oct},
  Pages                    = {224-230},

  Abstract                 = {This paper presents a new approach for robust open-loop and closed-loop control of nonlinear processes with parameter uncertainties and a comparison with classical concepts. The approach leads to trajectories that show small variations if uncertain parameters and uncertain initial conditions are present. The algorithm utilizes the Unscented Transformation. It allows a 2nd order approximation of the first two statistical moments of the system's output as a function of the stochastic system's state and uncertain model parameters. Because the numerical burden is low, it can be used for optimization based online closed-loop process control as well},
  Doi                      = {10.1109/CACSD-CCA-ISIC.2006.4776650},
  ISSN                     = {2165-3011},
  Keywords                 = {closed loop systems;nonlinear control systems;open loop systems;predictive control;robust control;statistical analysis;stochastic systems;closed-loop control;nonlinear processes;open-loop control;parameter uncertainties;robust model predictive control;statistical moments;stochastic system;unscented transformation;Biological system modeling;Mathematical model;Open loop systems;Predictive control;Predictive models;Process control;Process design;Robust control;Uncertain systems;Uncertainty}
}

@InProceedings{Hensman2013,
  Title                    = {{Gaussian Processes for Big Data}},
  Author                   = {Hensman, J. and Fusi, N. and Lawrence, N. D.},
  Booktitle                = {Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013},
  Year                     = {2013},
  Pages                    = {282--290}
}

@Article{Hoffman2013,
  Title                    = {{Stochastic Variational Inference}},
  Author                   = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {1303--1347},
  Volume                   = {14},

  Abstract                 = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1206.7051},
  Eprint                   = {1206.7051},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Hoffman et al., Stochastic Variational Inference, 2013.pdf:pdf},
  ISBN                     = {1532-4435},
  ISSN                     = {1532-4435}
}

@Article{Hong2014a,
  Title                    = {{Fast identification algorithms for Gaussian process model}},
  Author                   = {Hong, Xia and Gao, Junbin and Jiang, Xinwei and Harris, Chris J.},
  Journal                  = {Neurocomputing},
  Year                     = {2014},
  Pages                    = {25--31},
  Volume                   = {133},

  Doi                      = {10.1016/j.neucom.2013.11.035},
  Keywords                 = {Gaussian process,Kullback–Leibler divergence,Optimization},
  Publisher                = {Elsevier}
}

@Book{Horn1990,
  Title                    = {Matrix Analysis},
  Author                   = {Horn, R. A. and Johnson, C. R.},
  Publisher                = {Cambridge University Press},
  Year                     = {1990},

  ISBN                     = {978-0-521-38632-6}
}

@InProceedings{Hu2008,
  Title                    = {An HMM Compensation Approach for Dynamic Features Using Unscented Transformation and its Application to Noisy Speech Recognition},
  Author                   = {Y. Hu and Q. Huo},
  Booktitle                = {Chinese Spoken Language Processing, 2008. ISCSLP '08. 6th International Symposium on},
  Year                     = {2008},
  Month                    = {Dec},
  Pages                    = {1-4},

  Abstract                 = {In our previous work, a new HMM compensation approach for static MFCC features was proposed by using a technique called Unscented Transformation (UT). Three implementations of the UT approach with different computational complexities were evaluated on Aurora2 connected digits database, and significant performance improvements were achieved compared to log-normal- approximation-based PMC (Parallel Model Combination) and first- order-approximation-based VTS (Vector Taylor Series) approaches. In this paper, we extend our UT-based formulation to compensating for HMM parameters corresponding to both static and dynamic features. New experimental results on Aurora2 task are reported to demonstrate the effectiveness of the proposed UT approach.},
  Doi                      = {10.1109/CHINSL.2008.ECP.39},
  Keywords                 = {cepstral analysis;computational complexity;hidden Markov models;speech recognition;Aurora2 connected digit database;HMM compensation approach;computational complexities;mel-frequency cepstral coefficients;noisy speech recognition;static MFCC features;unscented transformation;Additive noise;Automatic speech recognition;Computational complexity;Gaussian noise;Hidden Markov models;Nonlinear distortion;Spatial databases;Speech enhancement;Speech recognition;Taylor series}
}

@Article{Huber2014,
  Title                    = {{Recursive Gaussian process: On-line regression and learning}},
  Author                   = {Huber, M. F.},
  Journal                  = {Pattern Recognition Letters},
  Year                     = {2014},
  Pages                    = {85--91},
  Volume                   = {45},

  Doi                      = {10.1016/j.patrec.2014.03.004},
  Publisher                = {Elsevier B.V.}
}

@InProceedings{Huber2013,
  Title                    = {{Recursive Gaussian process regression}},
  Author                   = {Huber, M. F.},
  Booktitle                = {IEEE International Conference on Acoustics, Speech and Signal Processing},
  Year                     = {2013},
  Pages                    = {3362--3366},
  Publisher                = {IEEE},

  Doi                      = {10.1109/ICASSP.2013.6638281}
}

@InProceedings{Huszar2012,
  Title                    = {{Optimally-Weighted Herding is Bayesian Quadrature}},
  Author                   = {Husz\'{a}r, F. and Duvenaud, D.},
  Booktitle                = {Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012},
  Year                     = {2012},
  Pages                    = {377--386},

  Abstract                 = {Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1204.1664},
  Eprint                   = {1204.1664},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Husz\'{a}r, Duvenaud, Optimally-Weighted Herding is Bayesian Quadrature, 2012.pdf:pdf},
  Mendeley-groups          = {Gaussian Processes/Quadrature}
}

@Book{Inzenman2008,
  Title                    = {{Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning}},
  Author                   = {Inzenman, A. J.},
  Publisher                = {Springer},
  Year                     = {2008},
  Pages                    = {733},

  ISBN                     = {978-0-387-78188-4}
}

@Article{Ito2000,
  Title                    = {{Gaussian Filters for Nonlinear Filtering Problems}},
  Author                   = {Ito, K. and Xiong, K.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2000},
  Number                   = {5},
  Pages                    = {910--927},
  Volume                   = {45},

  Annote                   = {UKF is a special case of Gaussian Filters.},
  Doi                      = {10.1109/9.855552}
}

@Book{Janson1997,
  Title                    = {{Gaussian Hilbert Spaces}},
  Author                   = {Janson, S.},
  Publisher                = {Cambridge University Press},
  Year                     = {1997},
  Pages                    = {352},

  ISBN                     = {978-0521057202}
}

@Book{Jazwinski1970,
  Title                    = {{Stochastic Processes and Filtering Theory}},
  Author                   = {Jazwinski, A. H.},
  Publisher                = {Academic Press},
  Year                     = {1970},
  Pages                    = {376},

  Address                  = {New York},
  ISBN                     = {978-0-486-46274-5}
}

@Article{Jiang2003,
  Title                    = {Kalman filtering for power estimation in mobile communications},
  Author                   = {Jiang, T. and Sidiropoulos, N.D. and Giannakis, G.B.},
  Journal                  = {Wireless Communications, IEEE Transactions on},
  Year                     = {2003},
  Number                   = {1},
  Pages                    = {151-161},
  Volume                   = {2},

  Doi                      = {10.1109/TWC.2002.806386}
}

@Book{Jordan1999,
  Title                    = {{Learning in Graphical Models}},
  Author                   = {Jordan, M. I.},
  Publisher                = {MIT Press},
  Year                     = {1999},
  Pages                    = {640},

  ISBN                     = {978-0262600323}
}

@InProceedings{Julier2002b,
  Title                    = {Reduced sigma point filters for the propagation of means and covariances through nonlinear transformations},
  Author                   = {Julier, S.J. and Uhlmann, J.K.},
  Booktitle                = {American Control Conference, 2002. Proceedings of the 2002},
  Year                     = {2002},
  Pages                    = {887-892 vol.2},
  Volume                   = {2},

  Doi                      = {10.1109/ACC.2002.1023128},
  ISSN                     = {0743-1619}
}

@InProceedings{Julier2002a,
  Title                    = {{The Scaled Unscented Transformation}},
  Author                   = {Julier, S. J.},
  Booktitle                = {Proceedings of the 2002 American Control Conference},
  Year                     = {2002},
  Pages                    = {4555--4559},
  Publisher                = {IEEE},

  Abstract                 = {This paper describes a generalisation of the unscented transformation (UT) which allows sigma points to be scaled to an arbitrary dimension. The UT is a method for predicting means and covariances in nonlinear systems. A set of samples are deterministically chosen which match the mean and covariance of a (not necessarily Gaussian-distributed) probability distribution. These samples can be scaled by an arbitrary constant. The method guarantees that the mean and covariance second order accuracy in mean and covariance, giving the same performance as a second order truncated filter but without the need to calculate any Jacobians or Hessians. The impacts of scaling issues are illustrated by considering conversions from polar to Cartesian coordinates with large angular uncertainties.}
}

@Article{Julier,
  Title                    = {{A New Extension of the Kalman Filter to Nonlinear Systems}},
  Author                   = {Julier, S. J. and Uhlmann, J. K.},

  Annote                   = {Introduces UKF}
}

@Article{Julier2004,
  Title                    = {{Unscented Filtering and Nonlinear Estimation}},
  Author                   = {Julier, S. J. and Uhlmann, J. K.},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {401--422},
  Volume                   = {92},

  Doi                      = {10.1109/JPROC.2003.823141},
  Keywords                 = {estimation,kalman filtering,nonlinear systems}
}

@InProceedings{Julier2002,
  Title                    = {{Reduced Sigma Point Filters for the Propagation of Means and Covariances Through Nonlinear Transformations}},
  Author                   = {Julier, S. J. and Uhlmann, J. K.},
  Booktitle                = {Proceedings of the 2002 American Control Conference},
  Year                     = {2002},
  Pages                    = {887--892},
  Publisher                = {American Automatic Control Council},
  Volume                   = {2},

  Doi                      = {10.1109/ACC.2002.1023128},
  Keywords                 = {kalman filter,non-linear estimation,unscented filtering}
}

@TechReport{Julier1996,
  Title                    = {{A General Method for Approximating Nonlinear Transformations of Probability Distributions}},
  Author                   = {Julier, S. J. and Uhlmann, J. K.},
  Institution              = {Robotics Research Group, Department of Engineering Science, University of Oxford},
  Year                     = {1996},

  Pages                    = {1--27}
}

@Article{Julier2000,
  Title                    = {{A New Method for the Nonlinear Transformation of Means and Covariances in Filters and Estimators}},
  Author                   = {Julier, S. J. and Uhlmann, J. K. and Durrant-Whyte, H. F.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {477--482},
  Volume                   = {45}
}

@Article{Kailath1974,
  Title                    = {{A View of Three Decades of Linear Filtering Theory}},
  Author                   = {Kailath, T.},
  Journal                  = {IEEE Transactions on Information Theory},
  Year                     = {1974},
  Number                   = {2},
  Pages                    = {146--181},
  Volume                   = {20},

  Doi                      = {10.1109/TIT.1974.1055174}
}

@Article{Kalman1960,
  Title                    = {{A New Approach to Linear Filtering and Prediction Problems}},
  Author                   = {Kalman, R. E.},
  Journal                  = {Journal of Basic Engineering},
  Year                     = {1960},
  Number                   = {1},
  Pages                    = {35--45},
  Volume                   = {82},

  Publisher                = {American Society of Mechanical Engineers}
}

@Article{Kalman1961a,
  Title                    = {{New Results in Linear Filtering and Prediction Theory}},
  Author                   = {Kalman, R. E. and Bucy, R. S.},
  Journal                  = {Journal of Basic Engineering},
  Year                     = {1961},
  Number                   = {1},
  Pages                    = {95},
  Volume                   = {83},

  Annote                   = {Kalman-Bucy filter},
  Doi                      = {10.1115/1.3658902}
}

@Article{Kaminski1971,
  Title                    = {{Discrete Square Root Filtering: A Survey of Current Techniques}},
  Author                   = {Kaminski, P. and Bryson, A. E. and Schmidt, S. F.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {1971},
  Number                   = {6},
  Pages                    = {736},
  Volume                   = {16}
}

@Article{Kanaga2016,
  Title                    = {{Convergence guarantees for kernel-based quadrature rules in misspecified settings}},
  Author                   = {Kanagawa, M. and Sriperumbudur, B. K. and Fukumizu, K.},
  Year                     = {2016},

  Abstract                 = {Kernel-based quadrature rules are powerful tools for numerical integration which yield con- vergence rates much faster than usual Mote Carlo methods. These rules are constructed based on the assumption that the integrand has a certain degree of smoothness, and this assump- tion is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, in practice such an assumption can be violated, and no general theory has been established for the convergence in such misspecified cases. In this paper, we prove that ker- nel quadrature rules can be consistent even when an integrand does not belong to an assumed RKHS, i.e., when the integrand is less smooth than assumed. We derive convergence rates that depend on the (unknown) smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces.},
  Annote                   = {Prooves guarantees for BQ rules with deterministic points, such as UT, SR and Gauss-Hermite.
Shows that convergence rates derived for smooth functions hold even for rougher functions.},
  Url                      = {https://arxiv.org/pdf/1605.07254v1.pdf}
}

@Article{Kennedy1998,
  Title                    = {{Bayesian quadrature with non-normal approximating functions}},
  Author                   = {Kennedy, M.},
  Journal                  = {Statistics and Computing},
  Year                     = {1998},
  Number                   = {4},
  Pages                    = {365},
  Volume                   = {8},

  Abstract                 = {We consider an efficient Bayesian approach to estimating integration-based posterior summaries from a separate Bayesian application. In Bayesian quadrature we model an intractable posterior density function f() as a Gaussian process, using an approximating function g(), and find a posterior distribution for the integral of f(), conditional on a few evaluations of f at selected design points. Bayesian quadrature using normal g is called Bayes-Hermite quadrature. We extend this theory by allowing g() to be chosen from two wider classes of functions. One is a family of skew densities and the other is the family of finite mixtures of normal densities. For the family of skew densities we describe an iterative updating procedure to select the most suitable approximation and apply the method to two simulated posterior density functions.},
  Doi                      = {10.1023/A:1008832824006},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Kennedy, Bayesian quadrature with non-normal approximating functions, 1998.pdf:pdf},
  Keywords                 = {approximating,bayesian quadrature,gaussian process,nite mixture normals,numerical integration,posterior densities},
  Mendeley-groups          = {Gaussian Processes/Quadrature}
}

@Article{Kerwin1999,
  Title                    = {{The Kriging Update Model and Recursive Space-Time Function Estimation}},
  Author                   = {Kerwin, W. S. and Prince, J. L.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {1999},
  Number                   = {11},
  Pages                    = {2942--2952},
  Volume                   = {47}
}

@Article{Kitagawa1996,
  Title                    = {Monte Carlo Filter and Smoother for Non-Gaussian Nonlinear State Space Models},
  Author                   = {Kitagawa, G.},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {1996},
  Number                   = {1},
  Pages                    = {1-25},
  Volume                   = {5},

  Abstract                 = {A new algorithm for the prediction, filtering, and smoothing of non-Gaussian nonlinear state space models is shown. The algorithm is based on a Monte Carlo method in which successive prediction, filtering (and subsequently smoothing), conditional probability density functions are approximated by many of their realizations. The particular contribution of this algorithm is that it can be applied to a broad class of nonlinear non-Gaussian higher dimensional state space models on the provision that the dimensions of the system noise and the observation noise are relatively low. Several numerical examples are shown.}
}

@Article{Kivinen2004a,
  Title                    = {{Online Learning with Kernels}},
  Author                   = {Kivinen, J. and a.J. Smola and Williamson, R.C.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2004},
  Number                   = {8},
  Pages                    = {2165--2176},
  Volume                   = {52},

  Doi                      = {10.1109/TSP.2004.830991}
}

@InProceedings{Ko2008,
  Title                    = {{GP-BayesFilters: Bayesian Filtering Using Gaussian Process Prediction and Observation Models}},
  Author                   = {Ko, J. and Fox, D.},
  Booktitle                = {Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on},
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {3471--3476},
  Publisher                = {Springer},
  Volume                   = {27},

  Abstract                 = {Bayesian filtering is a general framework for recursively estimating the state of a dynamical system. The most common instantiations of Bayes filters are Kalman filters (extended and unscented) and particle filters. Key components of each Bayes filter are probabilistic prediction and observation models. Recently, Gaussian processes have been introduced as a non-parametric technique for learning such models from training data. In the context of unscented Kalman filters, these models have been shown to provide estimates that can be superior to those achieved with standard, parametric models. In this paper we show how Gaussian process models can be integrated into other Bayes filters, namely particle filters and extended Kalman filters. We provide a complexity analysis of these filters and evaluate the alternative techniques using data collected with an autonomous micro-blimp.},
  Doi                      = {10.1109/IROS.2008.4651188},
  Keywords                 = {Localization,Visual Tracking}
}

@InProceedings{Ko2007,
  Title                    = {{GP-UKF: Unscented Kalman Filters with Gaussian Process Prediction and Observation Models}},
  Author                   = {Ko, J. and Klein, D. J. and Fox, D. and Haehnel, D.},
  Booktitle                = {Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference on},
  Year                     = {2007},
  Number                   = {3},
  Pages                    = {1901--1907},
  Publisher                = {IEEE},
  Volume                   = {54},

  Abstract                 = {This paper considers the use of non-parametric system models for sequential state estimation. In particular, motion and observation models are learned from training examples using Gaussian process (GP) regression. The state estimator is an unscented Kalman filter (UKF). The resulting GP-UKF algorithm has a number of advantages over standard (parametric) UKFs. These include the ability to estimate the state of arbitrary nonlinear systems, improved tracking quality compared to a parametric UKF, and graceful degradation with increased model uncertainty. These advantages stem from the fact that GPs consider both the noise in the system and the uncertainty in the model. If an approximate parametric model is available, it can be incorporated into the GP; resulting in further performance improvements. In experiments, we show how the GP-UKF algorithm can be applied to the problem of tracking an autonomous micro-blimp.},
  Doi                      = {10.1109/IROS.2007.4399284}
}

@Article{Kocijan2005,
  Title                    = {{Dynamic systems identification with Gaussian processes}},
  Author                   = {Kocijan, J. and Girard, A. and Banko, B. and Murray-Smith, R.},
  Journal                  = {Mathematical and Computer Modelling of Dynamic Systems},
  Year                     = {2005},
  Number                   = {4},
  Pages                    = {411--424},
  Volume                   = {11},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Kocijan et al., Dynamic systems identification with Gaussian processes, 2005.pdf:pdf},
  Url                      = {http://www.tandfonline.com/doi/abs/10.1080/13873950500068567}
}

@InProceedings{Kocijan2003,
  Title                    = {{Predictive control with Gaussian process models}},
  Author                   = {Kocijan, J. and Murray-Smith, R. and Rasmussen, C. E. and Likar, B.},
  Booktitle                = {EUROCON 2003. Computer as a Tool. The IEEE Region 8},
  Year                     = {2003},
  Pages                    = {352--356},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Kocijan et al., Predictive control with Gaussian process models, 2003.pdf:pdf},
  ISBN                     = {078037763X},
  Keywords                 = {constraint optimisation,gaussian,model based predictive control,nonlinear control,process models},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1248042}
}

@Article{Kotecha2003,
  Title                    = {{Gaussian particle filtering}},
  Author                   = {Kotecha, J. H. and Djuri\'{c}, P. M.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2003},
  Number                   = {10},
  Pages                    = {2592--2601},
  Volume                   = {51},

  Abstract                 = {Sequential Bayesian estimation for dynamic state space models involves recursive estimation of hidden states based on noisy observations. The update of filtering and predictive densities for nonlinear models with non-Gaussian noise using Monte Carlo particle filtering methods is considered. The Gaussian particle filter (GPF) is introduced, where densities are approximated as a single Gaussian, an assumption which is also made in the extended Kalman filter (EKF). It is analytically shown that, if the Gaussian approximations hold true, the GPF minimizes the mean square error of the estimates asymptotically. The simulations results indicate that the filter has improved performance compared to the EKF, especially for highly nonlinear models where the EKF can diverge},
  Doi                      = {10.1109/TSP.2003.816758},
  Keywords                 = {Dynamic state space models,Extended Kalman filter,Gaussian mixture,Gaussian mixture filter,Gaussian particle filter,Gaussian sum filter,Gaussian sum particle filter,Monte Carlo filters,Nonlinear non-Gaussian stochastic systems,Particle filters,Sequential Bayesian estimation,Sequential sampling methods,Unscented Kalman filter}
}

@Article{Kotecha2003a,
  Title                    = {{Gaussian sum particle filtering}},
  Author                   = {Kotecha, J. H. and Djuri\'{c}, P. M.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2003},
  Number                   = {10},
  Pages                    = {2602--2612},
  Volume                   = {51},

  Abstract                 = {We use the Gaussian particle filter to build several types of Gaussian sum particle filters. These filters approximate the filtering and predictive distributions by weighted Gaussian mixtures and are basically banks of Gaussian particle filters. Then, we extend the use of Gaussian particle filters and Gaussian sum particle filters to dynamic state space (DSS) models with non-Gaussian noise. With non-Gaussian noise approximated by Gaussian mixtures, the non-Gaussian noise models are approximated by banks of Gaussian noise models, and Gaussian mixture filters are developed using algorithms developed for Gaussian noise DSS models. As a result, problems involving heavy-tailed densities can be conveniently addressed. Simulations are presented to exhibit the application of the framework developed herein, and the performance of the algorithms is examined.},
  Doi                      = {10.1109/TSP.2003.816754},
  Keywords                 = {Dynamic state-space models,Extended Kalman filter,Gaussian mixture,Gaussian particle filter,Gaussian sum filter,Gaussian sum particle filter,Monte Carlo filters,Nonlinear non-Gaussian stochastic systems,Particle filters,Sequential Bayesian estimation,Sequential sampling methods}
}

@Article{Kou2013a,
  Title                    = {{Sparse Gaussian Process regression model based on ℓ 1/2 regularization}},
  Author                   = {Kou, P. and Gao, F.},
  Journal                  = {Applied Intelligence},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {669--681},
  Volume                   = {40},

  Doi                      = {10.1007/s10489-013-0482-0},
  Keywords                 = {gaussian process,machine learning,regression,regularization,sparse model}
}

@InProceedings{Kral2014,
  Title                    = {{Gaussian Process Based Dual Adaptive Control of Nonlinear Stochastic Systems}},
  Author                   = {Kr\'{a}l, L. and Pr\"{u}her, J. and \v{S}imandl, M.},
  Booktitle                = {Control and Automation (MED), 2014 22nd Mediterranean Conference of},
  Year                     = {2014},
  Pages                    = {1074--1079},

  __markedentry            = {[JPruher:1]},
  Doi                      = {10.1109/MED.2014.6961517},
  Keywords                 = {Adaptation models,Adaptive control,Gaussian processes,Nonlinear systems,Predictive models,Uncertainty,Vectors}
}

@Article{Kramer1988,
  Title                    = {{Recursive Bayesian Estimation using Piece-wise Constant Approximations}},
  Author                   = {Kramer, S. C. and Sorenson, H. W.},
  Journal                  = {Automatica},
  Year                     = {1988},
  Number                   = {6},
  Pages                    = {789--801},
  Volume                   = {24}
}

@Article{Kulhavy1992,
  Title                    = {{Recursive Nonlinear Estimation: Geometry of a Space of Posterior Densities}},
  Author                   = {Kulhav\'{y}, R.},
  Journal                  = {Automatica},
  Year                     = {1992},
  Number                   = {2},
  Pages                    = {313--323},
  Volume                   = {28},

  Doi                      = {10.1016/0005-1098(92)90118-Y},
  Keywords                 = {--parameter estimation,abstract--the paper establishes a,approximation theory,bayesian statistics,differential geometry,geometric formulation for,if a,nonlinear parameter estimation using,recursive algorithms,reduced statistics,sampled data systems}
}

@Article{Kushner1967,
  Title                    = {{Approximations to Optimal Nonlinear Filters}},
  Author                   = {Kushner, H},
  Journal                  = {Automatic Control, IEEE Transactions on},
  Year                     = {1967},
  Number                   = {5},
  Pages                    = {546--556},
  Volume                   = {12},

  Abstract                 = {Let the signal and noise processes be given as solutions to nonlinear stochastic differential equations. The optimal filter for the problem, derived elsewhere, is usually infinite dimensional. Several methods of obtaining possibly useful finite dimensional approximations are considered here, and some of the special problems of simulation are discussed. The numerical results indicate a number of useful features of the approximating filters and suggest methods of improvement. The paper is concerned with problems where the noise and nonlinear effects are much too large for the use of linearization methods, which for the simulated problem, at least, were useless.},
  Doi                      = {10.1109/TAC.1967.1098671},
  Keywords                 = {Nonlinear filtering,Optimal control}
}

@PhdThesis{Kuss2006,
  Title                    = {{Gaussian Process Models for Robust Regression, Classification and Reinforcement Learning}},
  Author                   = {Kuss, M.},
  School                   = {Technische Universit\"{a}t Darmstadt},
  Year                     = {2006}
}

@InCollection{Lazaro-Gredilla2012,
  Title                    = {{Bayesian Warped Gaussian Processes}},
  Author                   = {L\'{a}zaro-Gredilla, M.},
  Booktitle                = {Advances in Neural Information Processing Systems 25},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2012},
  Editor                   = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
  Pages                    = {1619--1627}
}

@TechReport{Lazaro-Gredilla2007a,
  Title                    = {{Sparse Spectral Sampling Gaussian Processes}},
  Author                   = {L\'{a}zaro-Gredilla, M. and Qui\~{n}onero-Candela, J.},
  Institution              = {Microsoft Research, Tech. Rep.},
  Year                     = {2007}
}

@Article{Lazaro-Gredilla2010a,
  Title                    = {{Sparse Spectrum Gaussian Process Regression}},
  Author                   = {L\'{a}zaro-Gredilla, M. and Qui\~{n}onero-Candela, J. and Rasmussen, C. E. and Figueiras-Vidal, A. R.},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2010},
  Pages                    = {1865--1881},
  Volume                   = {11},

  Keywords                 = {computational efficiency,gaussian process,power spectrum,probabilistic regression,sparse approximation}
}

@InCollection{Lawrence2003,
  Title                    = {{Fast Sparse Gaussian Process Methods: The Informative Vector Machine}},
  Author                   = {Lawrence, N. and Seeger, M. and Herbrich, R.},
  Booktitle                = {Advances in Neural Information Processing Systems 15},
  Publisher                = {MIT Press},
  Year                     = {2003},
  Editor                   = {Becker, S. and Thrun, S. and Obermayer, K.},
  Pages                    = {625--632},

  Abstract                 = {We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn dsparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n d2), and in large real-world classi cation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signi cantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (`error bars'), allows for Bayesian model selection and is less complex in implementation.}
}

@Article{Lawrence2012,
  Title                    = {{A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction : Insights and New Models}},
  Author                   = {Lawrence, N. D.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2012},
  Pages                    = {1609--1638},
  Volume                   = {13},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Lawrence, A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction Insights and New Models, 2012.pdf:pdf}
}

@Article{Lefebvre2002,
  Title                    = {{Comment on "A New Method for the Nonlinear Transformation of Means and Covariances in Filters and Estimators" [with authors' reply]}},
  Author                   = {Lefebvre, T. and Bruyninckx, H. and {De Schutter}, J.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2002},
  Number                   = {8},
  Pages                    = {1408--1409},
  Volume                   = {47},

  Annote                   = {MUST-READ! Establishes the correspondence between the Linear Regression KF and the Unscented KF.}
}

@Article{Lefebvre2004,
  Title                    = {{Kalman Filters for nonlinear systems: a comparison of performance}},
  Author                   = {Lefebvre, Tine and Bruyninckx, Herman and Schutter, Joris De},
  Journal                  = {International Journal of Control},
  Year                     = {2004},
  Number                   = {7},
  Pages                    = {639--653},
  Volume                   = {77},

  Doi                      = {10.1080/00207170410001704998},
  Publisher                = {Taylor \& Francis}
}

@Article{Leithead2003,
  Title                    = {{Direct identification of nonlinear structure using Gaussian process prior models}},
  Author                   = {Leithead, W. E. and Solak, E. and Leith, D. J.},
  Journal                  = {Proc. European Control Conference},
  Year                     = {2003},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Leithead, Solak, Leith, Direct identification of nonlinear structure using Gaussian process prior models, 2003.pdf:pdf},
  Keywords                 = {algorithm,gaussian,identification,nonlinear structure,process prior models},
  Url                      = {http://www.nt.ntnu.no/users/skoge/prost/proceedings/ecc03/pdfs/433.pdf}
}

@InProceedings{Li2006,
  Title                    = {{Measuring Estimator's Credibility: Noncredibility Index}},
  Author                   = {Li, X. R. and Zhao, Z.},
  Booktitle                = {Information Fusion, 2006 9th International Conference on},
  Year                     = {2006},
  Pages                    = {1--8},

  Doi                      = {10.1109/ICIF.2006.301770},
  Keywords                 = {credibility,estimation,performance evaluation}
}

@Book{Liu2008,
  Title                    = {{Monte Carlo Strategies in Scientific Computing}},
  Author                   = {Liu, J. S.},
  Publisher                = {Springer},
  Year                     = {2008},
  Pages                    = {364},

  ISBN                     = {978-0387763699}
}

@Article{Ljung2010,
  Title                    = {{Perspectives on system identification}},
  Author                   = {Ljung, L.},
  Journal                  = {Annual Reviews in Control},
  Year                     = {2010},

  Month                    = apr,
  Number                   = {1},
  Pages                    = {1--12},
  Volume                   = {34},

  Doi                      = {10.1016/j.arcontrol.2009.12.001},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Ljung, Perspectives on system identification, 2010.pdf:pdf},
  ISSN                     = {13675788},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S1367578810000027}
}

@InProceedings{Maciejowski2013,
  Title                    = {{Fault tolerant control using Gaussian processes and model predictive control}},
  Author                   = {Maciejowski, J. and Yang, X.},
  Booktitle                = {Control and Fault-Tolerant Systems (SysTol), 2013 Conference on},
  Year                     = {2013},
  Pages                    = {1--12},
  Publisher                = {IEEE}
}

@Book{MacKay2003a,
  Title                    = {{Information Theory, Inference, and Learning Algorithms}},
  Author                   = {MacKay, D. J. C.},
  Publisher                = {Cambridge University Press},
  Year                     = {2003},
  Pages                    = {640},

  ISBN                     = {978-0521642989}
}

@Misc{Marshall1954,
  Title                    = {{The Use of Multistage Sampling Schemes in Monte Carlo Computations.}},

  Author                   = {Marshall, A. W.},
  Year                     = {1954},

  Annote                   = {Intorduces Importance Sampling},
  Publisher                = {RAND Corporation},
  Url                      = {http://www.rand.org/pubs/papers/P531.html}
}

@Article{Maryak2004,
  Title                    = {{Use of the Kalman Filter for Inference in State-Space Models With Unknown Noise Distributions}},
  Author                   = {Maryak, J. L. and Spall, J. C. and Heydon, B. D.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2004},
  Number                   = {1},
  Pages                    = {87--90},
  Volume                   = {49},

  Doi                      = {10.1109/TAC.2003.821415}
}

@Book{Maybeck1982,
  Title                    = {{Stochastic Models, Estimation and Control: Volume 2}},
  Author                   = {Maybeck, P. S.},
  Publisher                = {Academic Press},
  Year                     = {1982},
  Pages                    = {306},

  ISBN                     = {978-0124110434}
}

@Book{Maybeck1982a,
  Title                    = {{Stochastic Models, Estimation and Control: Volume 3}},
  Author                   = {Maybeck, P. S.},
  Publisher                = {Academic Press},
  Year                     = {1982},
  Pages                    = {291},

  ISBN                     = {978-0124807037}
}

@Book{Maybeck1979,
  Title                    = {{Stochastic Models, Estimation and Control: Volume 1}},
  Author                   = {Maybeck, P. S.},
  Publisher                = {Academic Press},
  Year                     = {1979},
  Pages                    = {444},

  ISBN                     = {978-0124110427}
}

@Article{McHutchon2011a,
  Title                    = {{Gaussian process training with input noise}},
  Author                   = {McHutchon, A. and Rasmussen, C. E.},
  Journal                  = {\ldots Processing Systems},
  Year                     = {2011},
  Pages                    = {1--9}
}

@Article{McNamee1967,
  Title                    = {{Construction of fully symmetric numerical integration formulas}},
  Author                   = {McNamee, J. and Stenger, F.},
  Journal                  = {Numerische Mathematik},
  Year                     = {1967},
  Number                   = {4},
  Pages                    = {327--344},
  Volume                   = {10},

  Abstract                 = {The paper develops a construction for finding fully symmetric integration formulas of arbitrary degree 2k+1 in n-space such that the number of evaluation points is O((2n)k)/k!),n \to \infty. Formulas of degrees 3, 5, 7, 9, are relatively simple and are presented in detail. The method has been tested by obtaining some special formulas of degrees 7, 9 and 11 but these are not presented here.},
  Doi                      = {10.1007/BF02162032},
  ISSN                     = {0029599X}
}

@Article{Metropolis1949,
  Title                    = {{The monte carlo method}},
  Author                   = {Metropolis, N. and Ulam, S.},
  Journal                  = {\ldots the American statistical association},
  Year                     = {1949},
  Number                   = {247},
  Pages                    = {335--341},
  Volume                   = {44},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Metropolis, Ulam, The monte carlo method, 1949.pdf:pdf},
  Url                      = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1949.10483310}
}

@Book{Meyer2000,
  Title                    = {{Matrix Analysis and Applied Linear Algebra}},
  Author                   = {Meyer, C.},
  Publisher                = {Society for Industrial and Applied Mathematics},
  Year                     = {2000},
  Pages                    = {730},

  ISBN                     = {978-0898714548}
}

@TechReport{Minka2005,
  Title                    = {{Divergence measures and message passing}},
  Author                   = {Minka, T. P.},
  Year                     = {2005},

  Abstract                 = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Minka, Divergence measures and message passing, 2005.pdf:pdf},
  Pages                    = {MSR--TR--2005--173}
}

@InProceedings{Minka2001,
  Title                    = {{Expectation Propagation for Approximate Bayesian Inference}},
  Author                   = {Minka, T. P.},
  Booktitle                = {Proceedings of the Seventeenth conference on Uncertainty in Artificial Intelligence},
  Year                     = {2001},
  Pages                    = {362----369},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Minka, Expectation Propagation for Approximate Bayesian Inference, 2001.pdf:pdf}
}

@PhdThesis{Minka2001a,
  Title                    = {{A Family of Algorithms for Approximate Bayesian Inference}},
  Author                   = {Minka, T. P.},
  School                   = {MIT},
  Year                     = {2001}
}

@TechReport{Minka2000,
  Title                    = {{Deriving Quadrature Rules from Gaussian Processes}},
  Author                   = {Minka, T. P.},
  Institution              = {Statistics Department, Carnegie Mellon University, Tech. Rep},
  Year                     = {2000},

  Booktitle                = {\ldots Department, Carnegie Mellon University, Tech. Rep},
  Pages                    = {1--21}
}

@Book{Murphy2012,
  Title                    = {{Machine Learning: A Probabilistic Perspective}},
  Author                   = {Murphy, K. P.},
  Publisher                = {MIT Press},
  Year                     = {2012},
  Pages                    = {1071},

  ISBN                     = {978-0-262-01802-9}
}

@Article{Murray-Smith2005a,
  Title                    = {{Transformations of Gaussian process priors}},
  Author                   = {Murray-Smith, R. and Pearlmutter, B. A.},
  Journal                  = {Deterministic and Statistical Methods in \ldots},
  Year                     = {2005},
  Pages                    = {110--123},

  Annote                   = {Derivative and integral observations}
}

@InProceedings{Murray-Smith2002a,
  Title                    = {{Nonlinear adaptive control using non-parametric Gaussian process prior models}},
  Author                   = {Murray-Smith, R. and Sbarbaro, D.},
  Booktitle                = {15th Triennial World Congress of the International Federation of Automatic Control},
  Year                     = {2002},
  Publisher                = {IFAC}
}

@Article{Norgaard2000,
  Title                    = {{New developments in state estimation for nonlinear systems}},
  Author                   = {N{\o}rgaard, M. and Poulsen, N. K. and Ravn, O.},
  Journal                  = {Automatica},
  Year                     = {2000},
  Pages                    = {1627--1638},
  Volume                   = {36},

  Annote                   = {Divided Difference Filters (DD1, DD2)}
}

@PhdThesis{Neal1995,
  Title                    = {{Bayesian learning for neural networks}},
  Author                   = {Neal, R. M.},
  School                   = {University of Toronto},
  Year                     = {1995}
}

@InCollection{Nguyen2014,
  Title                    = {{Automated Variational Inference for Gaussian Process Models}},
  Author                   = {Nguyen, T. V. and Bonilla, E. V.},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2014},
  Editor                   = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q},
  Pages                    = {1404--1412}
}

@Article{Ni2011b,
  Title                    = {{Recursive GPR for nonlinear dynamic process modeling}},
  Author                   = {Ni, W. and Tan, S. K. and Ng, W. J.},
  Journal                  = {Chemical Engineering Journal},
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {636--643},
  Volume                   = {173},

  Doi                      = {10.1016/j.cej.2011.08.021},
  Keywords                 = {gaussian process regression,nonlinear system modeling},
  Publisher                = {Elsevier B.V.}
}

@Article{OHagan1992,
  Title                    = {{Some Bayesian Numerical Analysis}},
  Author                   = {O'Hagan, A.},
  Journal                  = {Bayesian Statistics},
  Year                     = {1992},
  Pages                    = {345--363},
  Volume                   = {4},

  Mendeley-groups          = {Gaussian Processes/Quadrature},
  Publisher                = {Oxford University Press}
}

@Article{OHagan1991,
  Title                    = {{Bayes-Hermite quadrature}},
  Author                   = {O'Hagan, A.},
  Journal                  = {Journal of Statistical Planning and Inference},
  Year                     = {1991},
  Number                   = {3},
  Pages                    = {245--260},
  Volume                   = {29},

  Abstract                 = {Bayesian quadrature treats the problem of numerical integration as one of statistical inference. A prior Gaussian process distribution is assumed for the integrand, observations arise from evaluating the integrand at selected points, and a posterior distribution is derived for the integrand and the integral. Methods are developed for quadrature in Rp. A particular application is integrating the posterior density arising from some other Bayesian analysis. Simulation results are presented, to show that the resulting Bayes–Hermite quadrature rules may perform better than the conventional Gauss–Hermite rules for this application. A key result is derived for product designs, which makes Bayesian quadrature practically useful for integrating in several dimensions. Although the method does not at present provide a solution to the more difficult problem of quadrature in high dimensions, it does seem to offer real improvements over existing methods in relatively low dimensions.},
  Doi                      = {10.1016/0378-3758(91)90002-V},
  Keywords                 = {Bayesian quadrature,Gaussian process,Gaussian quadrature,numerical integration,product rule}
}

@Article{OHagan1987,
  Title                    = {Monte Carlo is Fundamentally Unsound},
  Author                   = {O'Hagan, A.},
  Journal                  = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  Year                     = {1987},
  Number                   = {2/3},
  Pages                    = {247--249},
  Volume                   = {36},

  Owner                    = {JPruher},
  Timestamp                = {2015.07.30}
}

@Article{OHagan1978,
  Title                    = {{Curve Fitting and Optimal Design for Prediction}},
  Author                   = {O'Hagan, A.},
  Journal                  = {Journal of Royal Statistical Society},
  Year                     = {1978},
  Number                   = {1},
  Pages                    = {1--42},
  Volume                   = {40},

  Keywords                 = {model,multiple regression,multivariate,optimal design,regression}
}

@Article{Oates2016,
  Title                    = {{Probabilistic Integration and Intractable Distributions}},
  Author                   = {{Oates}, C.~J. and {Briol}, F.-X. and {Girolami}, M.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2016},

  Month                    = jun,

  Abstract                 = {This paper considers numerical approximation for integrals of the form $$\int f(x) p(\mathrm{d}x)$$ in the case where $f(x)$ is an expensive black-box function and $p(\mathrm{d}x)$ is an intractable distribution (meaning that it is accessible only through a finite collection of samples). Our proposal extends previous work that treats numerical integration as a problem of statistical inference, in that we model both $f$ as an a priori unknown random function and $p$ as an a priori unknown random distribution. The result is a posterior distribution over the value of the integral that accounts for these dual sources of approximation error. This construction is designed to enable the principled quantification and propagation of epistemic uncertainty due to numerical error through a computational pipeline. The work is motivated by such problems that occur in the Bayesian calibration of computer models.},
  Archiveprefix            = {arXiv},
  Eprint                   = {1606.06841},
  Keywords                 = {Statistics - Methodology},
  Primaryclass             = {stat.ME}
}

@Article{Oates2014a,
  Title                    = {{Control functionals for Monte Carlo integration}},
  Author                   = {Oates, C. J. and Girolami, M. and Chopin, N.},
  Year                     = {2014},
  Number                   = {1},
  Volume                   = {1},

  Abstract                 = {This paper introduces a novel class of estimators for Monte Carlo integration, that leverage gradient information in order to provide the improved estimator performance demanded by contemporary statistical applications. The proposed estimators, called "control functionals", achieve sub-root-\$n\$ convergence and often require orders of magnitude fewer simulations, compared with existing approaches, in order to achieve a fixed level of precision. We focus on a particular sub-class of estimators that permit an elegant analytic form and study their properties, both theoretically and empirically. Results are presented on Bayes-Hermite quadrature, hierarchical Gaussian process models and non-linear ordinary differential equation models, where in each case our estimators are shown to offer state of the art performance.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1410.2392},
  Eprint                   = {1410.2392},
  Keywords                 = {control variates,gradient methods,non-parametric regression,variance reduction}
}

@Article{Oates2015,
  Title                    = {{Frank-Wolfe Bayesian Quadrature : Probabilistic Integration with Theoretical Guarantees}},
  Author                   = {Oates, C. J. and Osborne, M. A and Girolami, M.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2015},
  Pages                    = {1--19},

  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1506.02681v1},
  Eprint                   = {arXiv:1506.02681v1},
  Mendeley-groups          = {Gaussian Processes/Quadrature},
  Url                      = {http://arxiv.org/abs/1506.02681}
}

@PhdThesis{Osborne2010,
  Title                    = {{Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature}},
  Author                   = {Osborne, M. A.},
  School                   = {University of Oxford},
  Year                     = {2010}
}

@InProceedings{Osborne2012,
  Title                    = {{Bayesian Quadrature for Ratios}},
  Author                   = {Osborne, M. A. and Garnett, R.},
  Booktitle                = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2012},
  Pages                    = {832--840}
}

@InProceedings{Osborne2012a,
  Title                    = {{Active Learning of Model Evidence Using Bayesian Quadrature}},
  Author                   = {Osborne, M. A. and Rasmussen, C. E. and Duvenaud, D. K. and Garnett, R. and Roberts, S. J.},
  Booktitle                = {Advances in Neural Information Processing Systems (NIPS)},
  Year                     = {2012},
  Pages                    = {46--54}
}

@Article{Perez-Cruz2013,
  Title                    = {{Gaussian Processes for Nonlinear Signal Processing: An overview of recent advances}},
  Author                   = {P\'{e}rez-Cruz, F and {Van Vaerenbergh}, S and Murillo-Fuentes, J. J. and L\'{a}zaro-Gredilla, M. and Santamar\'{\i}a, I.},
  Journal                  = {IEEE Signal Processing Magazine},
  Year                     = {2013},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {40--50},
  Volume                   = {30},

  Doi                      = {10.1109/MSP.2013.2250352}
}

@InProceedings{Park2013,
  Title                    = {{Learning Based Robot Control with Sequential Gaussian Process}},
  Author                   = {Park, S. and Mustafa, S. K. and Shimada, K.},
  Booktitle                = {Robotic Intelligence In Informationally Structured Space (RiiSS), 2013 IEEE Workshop on},
  Year                     = {2013},
  Pages                    = {120--127},
  Publisher                = {Ieee},

  Doi                      = {10.1109/RiiSS.2013.6607939}
}

@Article{Peterka1981,
  Title                    = {{Bayesian System Identification}},
  Author                   = {Peterka, V.},
  Journal                  = {Automatica},
  Year                     = {1981},
  Number                   = {1},
  Pages                    = {41--53},
  Volume                   = {17},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Peterka, Bayesian System Identification, 1981.pdf:pdf},
  Keywords                 = {bayesian statistics,identification,parameter estimation,prediction,probability,random processes,state estimation,stochastic systems},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0005109881900832}
}

@Article{Pillonetto2008,
  Title                    = {{Solutions of nonlinear control and estimation problems in reproducing kernel Hilbert spaces: Existence and numerical determination}},
  Author                   = {Pillonetto, Gianluigi},
  Journal                  = {Automatica},
  Year                     = {2008},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {2135--2141},
  Volume                   = {44},

  Doi                      = {10.1016/j.automatica.2007.12.005},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Pillonetto, Solutions of nonlinear control and estimation problems in reproducing kernel Hilbert spaces Existence and numerical determin.pdf:pdf},
  ISSN                     = {00051098},
  Keywords                 = {compact sets,estimation theory,nonlinear inverse problems,nonparametric identification,tikhonov regularization},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0005109808000277}
}

@Article{Pillonetto2014,
  Title                    = {{Kernel methods in system identification, machine learning and function estimation: A survey}},
  Author                   = {Pillonetto, G. and Dinuzzo, F. and Chen, T. and {De Nicolao}, G. and Ljung, L.},
  Journal                  = {Automatica},
  Year                     = {2014},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {657--682},
  Volume                   = {50},

  Doi                      = {10.1016/j.automatica.2014.01.001},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Pillonetto et al., Kernel methods in system identification, machine learning and function estimation A survey, 2014.pdf:pdf},
  ISSN                     = {00051098},
  Keywords                 = {linear system identification,model complexity selection,prediction error methods},
  Publisher                = {Elsevier Ltd},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S000510981400020X}
}

@Book{Poincare1896,
  Title                    = {{Calcul des probabilit\'{e}s}},
  Author                   = {Poincar\'{e}, H.},
  Publisher                = {Paris, Gauthier-Villars},
  Year                     = {1896},
  Pages                    = {352},

  Mendeley-groups          = {Gaussian Processes/Quadrature}
}

@Article{Prueher2015a,
  Title                    = {{Functional Dual Adaptive Control with Recursive Gaussian Process Model}},
  Author                   = {Pr{\"{u}}her, J. and Kr{\'{a}}l, L.},
  Journal                  = {Journal of Physics: Conference Series},
  Year                     = {2015},
  Pages                    = {012006},
  Volume                   = {659},

  __markedentry            = {[JPruher:1]},
  Abstract                 = {The paper deals with dual adaptive control problem, where the functional uncertainties in the system description are modelled by a non-parametric Gaussian process regression model. Current approaches to adaptive control based on Gaussian process models are severely limited in their practical applicability, because the model is re-adjusted using all the currently available data, which keeps growing with every time step. We propose the use of recursive Gaussian process regression algorithm for significant reduction in computational requirements, thus bringing the Gaussian process-based adaptive controllers closer to their practical applicability. In this work, we design a bi-criterial dual controller based on recursive Gaussian process model for discrete-time stochastic dynamic systems given in an affine-in-control form. Using Monte Carlo simulations, we show that the proposed controller achieves comparable performance with the full Gaussian process-based controller in terms of control quality while keeping the computational demands bounded.},
  Doi                      = {10.1088/1742-6596/659/1/012006},
  ISSN                     = {1742-6588},
  Keywords                 = {adaptive control,dual control,functional adaptive control,gaussian process model},
  Url                      = {http://stacks.iop.org/1742-6596/659/i=1/a=012006?key=crossref.55205a08e7a8cc55e320b8ff0d4c535e}
}

@InProceedings{Prueher2016a,
  Title                    = {{On the Use of Gradient Information in Gaussian Process Quadratures}},
  Author                   = {Pr{\"{u}}her, J. and S{\"{a}}rkk{\"{a}}, S.},
  Booktitle                = {Proceedings of the 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)},
  Year                     = {2016},
  Editor                   = {Uncini, A. and Diamantaras, K. and Palmieri, F.A. N. and Larsen, J.},

  __markedentry            = {[JPruher:1]},
  Abstract                 = {Gaussian process quadrature is a promising alternative Bayesian approach to numerical integration, which offers attractive advantages over its well-known classical counter- parts and is particularly useful in non-linear Kalman filtering context. We show how Gaussian process quadrature can be naturally extended to work with gradients of the integrand. These results are used to design quadrature-based moment transformations leveraging gradients for non-linear Kalman filtering. We theoretically analyze the connection between our proposed transform and the Taylor series based linear transform (i.e., the extended Kalman filter). Numerical experiments on common sensor network nonlinearities show that adding derivative information improves the resulting estimates.}
}

@InCollection{Prueher2016,
  Title                    = {{Bayesian Quadrature Variance in Sigma-point Filtering}},
  Author                   = {Pr\"{u}her, J. and \v{S}imandl, M.},
  Booktitle                = {Informatics in Control, Automation and Robotics, 12th International Conference, ICINCO 2015 Colmar, Alsace, France, 21-23 July, 2015 Revised Selected Papers},
  Publisher                = {Springer International Publishing},
  Year                     = {2016},
  Editor                   = {Filipe, J. and Gusikhin, O. and Madani K. and Sasiadek, J.},
  Series                   = {Lecture Notes in Electrical Engineering},
  Volume                   = {370},

  __markedentry            = {[JPruher:1]},
  Owner                    = {JPruher},
  Timestamp                = {2015.10.23}
}

@InProceedings{Prueher2015,
  Title                    = {{Bayesian Quadrature in Nonlinear Filtering}},
  Author                   = {Pr\"{u}her, J. and \v{S}imandl, M.},
  Booktitle                = {Informatics in Control, Automation and Robotics (ICINCO), 2015 12th International Conference on},
  Year                     = {2015},
  Note                     = {To appear},

  __markedentry            = {[JPruher:1]},
  Owner                    = {JPruher},
  Timestamp                = {2015.06.10}
}

@Article{Prueher2014,
  Title                    = {{Gaussian process based recursive system identification}},
  Author                   = {Pr\"{u}her, J. and \v{S}imandl, M.},
  Journal                  = {Journal of Physics: Conference Series},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {1--9},
  Volume                   = {570},

  __markedentry            = {[JPruher:1]},
  Booktitle                = {11th European Workshop on Advanced Control and Diagnosis 2014, ACD 2014},
  Doi                      = {10.1088/1742-6596/570/1/012002}
}

@Article{Quinonero-Candela2005,
  Title                    = {{A Unifying View of Sparse Approximate Gaussian Process Regression}},
  Author                   = {Qui\~{n}onero-Candela, J. and Rasmussen, C. E.},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2005},
  Pages                    = {1939--1959},
  Volume                   = {6},

  Annote                   = {Great overview of the sparse GP approximations},
  Keywords                 = {bayesian committee,gaussian process,probabilistic regression,sparse approximation}
}

@InCollection{Quinonero-Candela2007,
  Title                    = {{Approximation Methods for Gaussian Process Regression}},
  Author                   = {Qui\~{n}onero-Candela, J. and Rasmussen, C. E. and Williams, C. K. I.},
  Booktitle                = {Large-Scale Kernel Machines},
  Publisher                = {MIT Press},
  Year                     = {2007},
  Chapter                  = {9},
  Editor                   = {Bottou, L. and Chapelle, O. and DeCoste, D. and Weston, J.},
  Pages                    = {203--223}
}

@Book{Rabinowitz2007,
  Title                    = {{Methods of Numerical Integration}},
  Author                   = {Rabinowitz, P. and Davis, P. J.},
  Publisher                = {Dover},
  Year                     = {2007},
  Pages                    = {624},

  ISBN                     = {978-0486453392}
}

@Article{Ranganathan2011,
  Title                    = {{Online Sparse Gaussian Process Regression and Its Applications}},
  Author                   = {Ranganathan, A. and Yang, M. H. and Ho, J.},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {391--404},
  Volume                   = {20},

  Annote                   = {OSMGP (Online Sparse Matrix Gaussian Process) and application to head tracking from image sequence.}
}

@InCollection{Rasmussen1999,
  Title                    = {{The Infinite Gaussian Mixture Model}},
  Author                   = {Rasmussen, C. E.},
  Booktitle                = {Advances in Neural Information Processing Systems 12},
  Publisher                = {MIT Press},
  Year                     = {1999},
  Editor                   = {Solla, S. A. and Leen, T. K. and M\"{u}ller, K. R.},
  Pages                    = {554--560},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Rasmussen, The Infinite Gaussian Mixture Model., 1999.pdf:pdf}
}

@InProceedings{Rasmussen2003a,
  Title                    = {{Bayesian Monte Carlo}},
  Author                   = {Rasmussen, C. E. and Ghahramani, Z.},
  Booktitle                = {Advances in Neural Information Processing Systems 15},
  Year                     = {2003},
  Editor                   = {Becker, S. and Thrun, S. and Obermayer, K.},
  Number                   = {1},
  Pages                    = {505--512},
  Publisher                = {MIT Press}
}

@Article{Rasmussen2003,
  Title                    = {{Gaussian Processes in Reinforcement Learning.}},
  Author                   = {Rasmussen, C. E. and Kuss, M.},
  Journal                  = {NIPS},
  Year                     = {2003},
  Pages                    = {1},
  Volume                   = {4}
}

@Book{Rasmussen2006,
  Title                    = {{Gaussian Processes for Machine Learning}},
  Author                   = {Rasmussen, C. E. and Williams, C. K.},
  Publisher                = {The MIT Press},
  Year                     = {2006},
  Pages                    = {248},

  ISBN                     = {978-0-262-18253-9}
}

@Article{Rauch1965,
  Title                    = {{Maximum Likelihood Estimates of Linear Dynamic Systems}},
  Author                   = {Rauch, H. E. and Striebel, C. T. and Tung, F.},
  Journal                  = {AIAA journal},
  Year                     = {1965},
  Number                   = {8},
  Pages                    = {1445----1450},
  Volume                   = {3}
}

@InProceedings{Reece2010,
  Title                    = {{An Introduction to Gaussian Processes for the Kalman Filter Expert}},
  Author                   = {Reece, S. and Roberts, S.},
  Booktitle                = {Proceedings of the 13th International Conference on Information Fusion},
  Year                     = {2010},
  Pages                    = {1-9},
  Publisher                = {IEEE},

  Keywords                 = {bayesian methods,gaussian processes,kalman filter,kriged,near constant acceleration}
}

@Article{Rhudy2013a,
  Title                    = {{Online Stochastic Convergence Analysis of the Kalman Filter}},
  Author                   = {Rhudy, M. B. and Gu, Y.},
  Journal                  = {International Journal of Stochastic Analysis},
  Year                     = {2013},
  Pages                    = {1--9},
  Volume                   = {2013},

  Doi                      = {10.1155/2013/240295}
}

@Book{Ristic2004,
  Title                    = {Beyond the Kalman Filter: Particle Filters for Tracking Applications},
  Author                   = {Ristic, B. A and Arulampalam, S. and Gordon, N.},
  Publisher                = {Artech Print},
  Year                     = {2004},
  Pages                    = {318},
  Series                   = {Artech House Radar Library},

  ISBN                     = {978-1580536318},
  Owner                    = {JPruher},
  Timestamp                = {2015.10.07}
}

@Book{Rogers2011,
  Title                    = {{A First Course in Machine Learning}},
  Author                   = {Rogers, S. and Girolami, M.},
  Publisher                = {CRC Press},
  Year                     = {2011},
  Pages                    = {285},

  ISBN                     = {978-1-4398-2414-6}
}

@InProceedings{Ross2015,
  Title                    = {Unscented guidance},
  Author                   = {I. M. Ross and R. J. Proulx and M. Karpenko},
  Booktitle                = {2015 American Control Conference (ACC)},
  Year                     = {2015},
  Month                    = {July},
  Pages                    = {5605-5610},

  Abstract                 = {Unscented guidance combines the concept of the unscented transform with standard optimal control theory to produce an open-loop method to manage uncertainties in nonlinear dynamical systems. The theoretical foundations for this concept can be framed in terms of a low-order semi-discretization of a Lebesgue-Stieltjes optimal control problem. A Zermelo problem subject to parameter uncertainties is used to quantitatively demonstrate the substantial reductions in target error statistics (95% in mean and 70% in the trace of the covariance) through the use of unscented guidance. These ideas generate a tractable approach for solving chance-constrained optimal control problems.},
  Doi                      = {10.1109/ACC.2015.7172217},
  ISSN                     = {0743-1619},
  Keywords                 = {error statistics;nonlinear dynamical systems;open loop systems;optimal control;Lebesgue-Stieltjes optimal control problem;Zermelo problem;chance-constrained optimal control problems;low-order semidiscretization;nonlinear dynamical systems;open-loop method;parameter uncertainty;standard optimal control theory;target error statistics;uncertainty management;unscented guidance;unscented transform concept;Error analysis;Monte Carlo methods;Optimal control;Standards;Trajectory;Transforms;Uncertainty}
}

@InProceedings{Roth2011,
  Title                    = {{An efficient implementation of the second order extended Kalman filter}},
  Author                   = {Roth, M. and Gustafsson, F.},
  Booktitle                = {Information Fusion (FUSION), 2011 Proceedings of the 14th International Conference on},
  Year                     = {2011},
  Pages                    = {1--6}
}

@Article{Roweis1999,
  Title                    = {{A Unifying Review of Linear Gaussian Models}},
  Author                   = {Roweis, S. and Ghahramani, Z.},
  Journal                  = {Neural Computation},
  Year                     = {1999},
  Number                   = {2},
  Pages                    = {305--345},
  Volume                   = {11},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Roweis, Ghahramani, A Unifying Review of Linear Gaussian Models, 1999.pdf:pdf},
  Publisher                = {MIT Press}
}

@Article{Saerkkae2016,
  Title                    = {{On the relation between Gaussian process quadratures and sigma-point methods}},
  Author                   = {{S{\"a}rkk{\"a}}, S. and {Hartikainen}, J. and {Svensson}, L. and {Sandblom}, F.},
  Journal                  = {Journal of Advances in Information Fusion},
  Year                     = {2016},

  Month                    = {June},
  Pages                    = {31-46},
  Volume                   = {11},

  ISSN                     = {1557-6418},
  Url                      = {http://arxiv.org/abs/1504.05994}
}

@Book{Saerkkae2013,
  Title                    = {{Bayesian Filtering and Smoothing}},
  Author                   = {S\"{a}rkk\"{a}, S.},
  Publisher                = {Cambridge University Press},
  Year                     = {2013},
  Pages                    = {252},

  Address                  = {New York},
  ISBN                     = {978-1-107-61928-9}
}

@Article{Saerkkae2012,
  Title                    = {{Infinite-dimensional Kalman filtering approach to spatio-temporal Gaussian process regression}},
  Author                   = {S\"{a}rkk\"{a}, S. and Hartikainen, J.},
  Journal                  = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2012},
  Pages                    = {993--1001}
}

@InProceedings{Sarkka2014,
  Title                    = {{Gaussian Process Quadratures in Nonlinear Sigma-Point Filtering and Smoothing}},
  Author                   = {S\"{a}rkk\"{a}, S. and Hartikainen, J. and Svensson, L. and Sandblom, F.},
  Booktitle                = {Information Fusion (FUSION), 2014 17th International Conference on},
  Year                     = {2014},
  Pages                    = {1--8},

  Annote                   = {Probabilistic numerics applied in nonlinear filtering and smoothing setting. Gaussian process used to ease the computation of the integrals.}
}

@Article{Saerkkae2013a,
  Title                    = {{Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering}},
  Author                   = {S\"{a}rkk\"{a}, S. and Solin, A. and Hartikainen, J.},
  Journal                  = {IEEE Signal Processing Magazine},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {51--61},
  Volume                   = {30},

  Abstract                 = {Gaussian process-based machine learning is a powerful Bayesian paradigm for nonparametric nonlinear regression and classification. In this article, we discuss connections of Gaussian process regression with Kalman filtering and present methods for converting spatiotemporal Gaussian process regression problems into infinite-dimensional state-space models. This formulation allows for use of computationally efficient infinite-dimensional Kalman filtering and smoothing methods, or more general Bayesian filtering and smoothing methods, which reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The implication of this is that the use of machine-learning models in signal processing becomes computationally feasible, and it opens the possibility to combine machine-learning techniques with signal processing methods.},
  Annote                   = {State space representation of Gaussian process models. Reduction in complexity.},
  Doi                      = {10.1109/MSP.2013.2246292},
  Keywords                 = {Bayes methods,Gaussian process regression problems,Gaussian process-based machine learning,Gaussian processes,Kalman filters,Kernel,Learning systems,Linear regression analysis,Machine learning,Parametric statistics,Smoothing methods,Spatiotemporal phenomena,infinite-dimensional Bayesian filtering,infinite-dimensional Kalman filtering,infinite-dimensional smoothing methods,infinite-dimensional state-space models,learning (artificial intelligence),linear time complexity,nonparametric nonlinear classification,nonparametric nonlinear regression,problematic cubic complexity,regression analysis,signal classification,signal processing methods,spatiotemporal learning},
  Shorttitle               = {Signal Processing Magazine, IEEE}
}

@Article{Sandblom2012,
  Title                    = {{Moment Estimation Using a Marginalized Transform}},
  Author                   = {Sandblom, F. and Svensson, L.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2012},
  Number                   = {12},
  Pages                    = {6138--6150},
  Volume                   = {60}
}

@InProceedings{Sandblom2011a,
  Title                    = {{Marginalized Sigma-Point Filtering}},
  Author                   = {Sandblom, F. and Svensson, L.},
  Booktitle                = {Information Fusion (FUSION), 2011 Proceedings of the 14th International Conference on},
  Year                     = {2011},
  Pages                    = {1--8},

  Keywords                 = {bayesian estimation,kalman filtering,moment matching,numerical integration,sigma point filtering}
}

@Article{Sarmavuori2012,
  Title                    = {{Fourier-Hermite Kalman Filter}},
  Author                   = {Sarmavuori, J. and S\"{a}rkk\"{a}, S.},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {1511--1515},
  Volume                   = {57}
}

@InProceedings{Savin2013,
  Title                    = {Covariance based uncertainty analysis with unscented transformation},
  Author                   = {A. A. Savin and V. G. Guba and B. D. Maxson},
  Booktitle                = {Microwave Measurement Conference, 2013 82nd ARFTG},
  Year                     = {2013},
  Month                    = {Nov},
  Pages                    = {1-4},

  Abstract                 = {Application of the unscented transformation (UT) and higher order unscented transformation (HOUT) are considered for uncertainty analysis. Using the principle that a set of discretely sampled points can be used to calculate mean and covariance, we can analyze nonlinear systems without the linearization steps and without defining the Jacobian matrix. An important example is presented.},
  Doi                      = {10.1109/ARFTG-2.2013.6737337},
  Keywords                 = {covariance analysis;measurement uncertainty;covariance based uncertainty analysis;measurement uncertainty;nonlinear systems;standard deviation;unscented transformation;Covariance matrices;Jacobian matrices;Measurement uncertainty;Monte Carlo methods;Standards;Uncertainty;Vectors;Measurement uncertainty;standard deviation;unscented transformation}
}

@InCollection{Sbarbaro2005,
  Title                    = {{Self-tuning Control of Non-linear Systems Using Gaussian Process Prior Models}},
  Author                   = {Sbarbaro, D. and Murray-Smith, R.},
  Booktitle                = {Switching and Learning in Feedback Systems},
  Publisher                = {Springer},
  Year                     = {2003},

  Address                  = {Berlin Heidelberg},
  Editor                   = {Murray-Smith, R. and Shorten, R.},
  Pages                    = {140--157},

  Doi                      = {10.1007/978-3-540-30560-6\_6}
}

@Book{Scholkopf2002,
  Title                    = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
  Author                   = {Sch\"{o}lkopf, B. and Smola, A. J.},
  Publisher                = {MIT Press},
  Year                     = {2002},
  Pages                    = {626},

  ISBN                     = {978-0-262-19475-4}
}

@InProceedings{Schoukens2009,
  Title                    = {{Wiener-Hammerstein Benchmark}},
  Author                   = {Schoukens, J. and Suykens, J. and Ljung, L.},
  Booktitle                = {Proceedings of the 15th IFAC Symposium on System Identification (SYSID 2009)},
  Year                     = {2009},
  Pages                    = {182},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Schoukens, Suykens, Ljung, Wiener-Hammerstein Benchmark, 2009.pdf:pdf},
  Keywords                 = {benchmark examples,nonlinear systems,system identification}
}

@InCollection{Schwaighofer2003,
  Title                    = {{Transductive and Inductive Methods for Approximate Gaussian Process Regression}},
  Author                   = {Schwaighofer, A. and Tresp, V.},
  Booktitle                = {Advances in Neural Information Processing Systems 15},
  Publisher                = {MIT Press},
  Year                     = {2003},
  Pages                    = {977--984}
}

@Book{Searle1982,
  Title                    = {{Matrix Algebra Useful For Statistic}},
  Author                   = {Searle, S. R.},
  Publisher                = {Wiley},
  Year                     = {1982},
  Pages                    = {464},

  ISBN                     = {978-0471866817}
}

@Article{Seeger2003,
  Title                    = {{Fast forward selection to speed up sparse Gaussian process regression}},
  Author                   = {Seeger, M and Williams, Cki and Lawrence, Nd},
  Journal                  = {Workshop on AI and Statistics},
  Year                     = {2003},
  Pages                    = {2003},
  Volume                   = {9},

  Abstract                 = {We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the "support" patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a suciently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically.}
}

@Article{Silverman1985,
  Title                    = {{Some Aspects of the Spline Smoothing Approach to Non-parametric Regression Curve Fitting (with discussion)}},
  Author                   = {Silverman, B. W.},
  Journal                  = {Journal of Royal Statistical Society B},
  Year                     = {1985},
  Number                   = {1},
  Pages                    = {1-52},
  Volume                   = {47},

  Owner                    = {JPruher},
  Timestamp                = {2015.06.04}
}

@Book{Simon2006,
  Title                    = {{Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches}},
  Author                   = {Simon, D.},
  Publisher                = {Wiley-Blackwell},
  Year                     = {2006},
  Pages                    = {552},

  ISBN                     = {978-0471708582}
}

@TechReport{Smith1962,
  Title                    = {{Application of statistical filter theory to the optimal estimation of position and velocity on board a circumlunar vehicle}},
  Author                   = {Smith, G. L. and Schmidt, S. F. and McGee, L. A.},
  Institution              = {NASA Ames Research Center: Technical Report R-135},
  Year                     = {1962}
}

@InCollection{Smola2001,
  Title                    = {{Sparse Greedy Gaussian Process Regression}},
  Author                   = {Smola, A. J. and Bartlett, P.},
  Booktitle                = {Advances in Neural Information Processing Systems 13},
  Publisher                = {MIT Press},
  Year                     = {2001},
  Editor                   = {Leen, T.K. and Dietterich, T.G. and Tresp, V.},
  Pages                    = {619--625}
}

@InCollection{Snelson2006,
  Title                    = {{Sparse Gaussian Processes using Pseudo-inputs}},
  Author                   = {Snelson, E. and Ghahramani, Z.},
  Booktitle                = {Advances in Neural Information Processing Systems 18},
  Publisher                = {MIT Press},
  Year                     = {2006},
  Editor                   = {Weiss, Y. and Sch\"{o}lkopf, B. and Platt, J. C.},
  Pages                    = {1257--1264}
}

@InCollection{Snelson2004,
  Title                    = {{Warped Gaussian Processes}},
  Author                   = {Snelson, E. and Ghahramani, Z. and Rasmussen, C. E.},
  Booktitle                = {Advances in Neural Information Processing Systems 16},
  Publisher                = {MIT Press},
  Year                     = {2004},
  Editor                   = {Thrun, S. and Saul, L. K. and Sch\"{o}lkopf, B.},
  Pages                    = {337--344}
}

@Article{Snyder2008,
  Title                    = {{Obstacles to High-Dimensional Particle Filtering}},
  Author                   = {Snyder, C. and Bengtsson, T and Bickel, P. and Anderson, J.},
  Journal                  = {Monthly Weather Review},
  Year                     = {2008},
  Number                   = {12},
  Pages                    = {4629--4640},
  Volume                   = {136},

  Doi                      = {10.1175/2008MWR2529.1}
}

@InProceedings{Soh2012b,
  Title                    = {{Iterative Temporal Learning and Prediction with the Sparse Online Echo State Gaussian Process}},
  Author                   = {Soh, H. and Demiris, Y.},
  Booktitle                = {The 2012 International Joint Conference on Neural Networks (IJCNN)},
  Year                     = {2012},
  Pages                    = {1--8},
  Publisher                = {Ieee},

  Doi                      = {10.1109/IJCNN.2012.6252504}
}

@InProceedings{Solak2003a,
  Title                    = {{Derivative Observations in Gaussian Process Models of Dynamic Systems}},
  Author                   = {Solak, E. and Murray-Smith, R. and Leithead, W. E.},
  Booktitle                = {Advances in Neural Information Processing Systems 15},
  Year                     = {2003},
  Editor                   = {Becker, S. and Thrun, S. and Obermayer, K.},
  Pages                    = {1057--1064},
  Publisher                = {MIT Press}
}

@Article{Sorenson1970,
  Title                    = {{Least-squares estimation: from Gauss to Kalman}},
  Author                   = {Sorenson, H. W.},
  Journal                  = {IEEE Spectrum},
  Year                     = {1970},
  Pages                    = {63--68}
}

@Book{Stein1999,
  Title                    = {{Interpolation of Spatial Data}},
  Author                   = {Stein, M. L.},
  Publisher                = {Springer-Verlag},
  Year                     = {1999},

  ISBN                     = {978-1-4612-1494-6}
}

@InCollection{Steinberg2014a,
  Title                    = {{Extended and Unscented Gaussian Processes}},
  Author                   = {Steinberg, D. M. and Bonilla, E. V.},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2014},
  Editor                   = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N.D. and Weinberger, K.Q.},
  Pages                    = {1251--1259}
}

@InProceedings{Steinbring2013,
  Title                    = {{S2KF : The Smart Sampling Kalman Filter}},
  Author                   = {Steinbring, J. and Hanebeck, U. D.},
  Booktitle                = {Information Fusion (FUSION), 2013 16th International Conference on},
  Year                     = {2013},
  Pages                    = {2089--2096}
}

@Article{Stigler,
  Title                    = {{The Epic Story of Maximum Likelihood}},
  Author                   = {Stigler, S. M.},

  Archiveprefix            = {arXiv},
  Arxivid                  = {0804.2996},
  Eprint                   = {0804.2996},
  Keywords                 = {Methodology},
  Url                      = {http://arxiv.org/abs/0804.2996}
}

@InProceedings{Straka2014,
  Title                    = {{Comparison of Adaptive and Randomized Unscented Kalman Filter Algorithms}},
  Author                   = {Straka, O. and Dun\'{\i}k, J. and \v{S}imandl, M. and Blasch, E.},
  Booktitle                = {Information Fusion (FUSION), 2014 17th International Conference on},
  Year                     = {2014},
  Pages                    = {1--8},

  Keywords                 = {estimation theory,filters,kalman filtering,nonlinear,state estimation,unscented kalman filter}
}

@InProceedings{Sun2014,
  Title                    = {Trajectory planning for vehicle autonomous driving with uncertainties},
  Author                   = {Hao Sun and Weiwen Deng and Sumin Zhang and Shanshan Wang and Yutan Zhang},
  Booktitle                = {Informative and Cybernetics for Computational Social Systems (ICCSS), 2014 International Conference on},
  Year                     = {2014},
  Month                    = {Oct},
  Pages                    = {34-38},

  Abstract                 = {This paper proposes a novel method on dynamic trajectory planning for intelligent vehicle driving under traffic environment with uncertainties. The statistical characteristics of traffic vehicle motion are first analyzed with a traffic vehicle model, in which the inputs are considered to be random variables with certain probability distribution. Therefore the output of the model can be calculated via unscented transformation for probabilistic spread. Then the overall collision probability of the candidate trajectories is assessed with certain confidence level. Finally a trajectory planning method is employed to achieve multiple objectives for lane change maneuver with combined efficiency and comfort. Simulation is conducted with results demonstrating that the proposed method is valid and effective.},
  Doi                      = {10.1109/ICCSS.2014.6961812},
  Keywords                 = {path planning;road traffic control;road vehicles;statistical distributions;trajectory control;collision probability;dynamic trajectory planning;intelligent vehicle driving;lane change maneuver;probability distribution;traffic environment;traffic vehicle model;traffic vehicle motion;uncertainties;vehicle autonomous driving;Acceleration;Cybernetics;Planning;Trajectory;Uncertainty;Vehicle dynamics;Vehicles;autonomous driving;trajectory planning;uncertainties}
}

@Book{Tanizaki1996,
  Title                    = {{Nonlinear Filters: Estimation and Applications}},
  Author                   = {Tanizaki, H.},
  Publisher                = {Springer-Verlag},
  Year                     = {1996},

  Edition                  = {2nd},
  Pages                    = {256},

  ISBN                     = {978-3-540-61326-8}
}

@Book{Theodoridis2015,
  Title                    = {Machine Learning: A Bayesian and Optimization Perspective},
  Author                   = {Theodoridis, S.},
  Publisher                = {Academic Press},
  Year                     = {2015},
  Pages                    = {1062},

  Owner                    = {JPruher},
  Timestamp                = {2015.10.07}
}

@InProceedings{Titsias2009,
  Title                    = {{Variational Learning of Inducing Variables in Sparse Gaussian Processes}},
  Author                   = {Titsias, Michalis},
  Booktitle                = {Proceedings of the 12th International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},
  Pages                    = {567--574},
  Volume                   = {5},

  Abstract                 = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Titsias, Variational Learning of Inducing Variables in Sparse Gaussian Processes, 2009.pdf:pdf},
  Keywords                 = {Learning/Statistics \& Optimisation,Theory \& Algorithms},
  Mendeley-groups          = {Gaussian Processes/Approximations}
}

@Article{Tresp2000,
  Title                    = {{A Bayesian Commitee Machine}},
  Author                   = {Tresp, V.},
  Journal                  = {Neural Computation},
  Year                     = {2000},
  Pages                    = {2719--2741},
  Volume                   = {12}
}

@InProceedings{Tronarp2016,
  Title                    = {Sigma-point Filtering for Nonlinear Systems with Non-additive Heavy-tailed Noise},
  Author                   = {Tronarp, F. and Hostettler, R. and S\"{a}rkk\"{a}, S.},
  Booktitle                = {2016 19th International Conference on Information Fusion (FUSION)},
  Year                     = {2016},
  Month                    = {July},
  Pages                    = {1859-1866},

  Abstract                 = {This paper is concerned with sigma-point methods for filtering in nonlinear systems, where the process and measurement noise are heavy tailed and enter the system non-additively. The problem is approached within the framework of assumed density filtering and the necessary statistics are approximated using sigma-point methods developed for Student's t-distribution. This leads to UKF/CKF-type of filters for Student's t-distribution. Four different sigma-point methods are considered that compute exact expectations of polynomials for orders up to 3, 5, 7, and 9, respectively. The resulting algorithms are evaluated in a simulation example and real data from a pedestrian dead-reckoning experiment. In the simulation experiment the nonlinear Student's t filters are found to be faster in suppressing large errors in the state estimates in comparison to the UKF when filtering in nonlinear Gaussian systems with outliers in process and measurement noise. In the pedestrian dead-reckoning experiment the sigma-point Student's t filter was found to yield better loop closure and path length estimates as well as significantly improved robustness towards extreme accelerometer measurement spikes.}
}

@InProceedings{Turner2010b,
  Title                    = {{Model Based Learning of Sigma Points in Unscented Kalman Filtering}},
  Author                   = {Turner, R. and Rasmussen, C. E.},
  Booktitle                = {Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop on},
  Year                     = {2010},
  Pages                    = {178--183},

  Annote                   = {Finding optimal settings of UKF parameters through maximizing log marginal likelihood of the UKF. Maximization of log ML is achieved through GP optimization, i.e. GP is used as a surrogate model of logML in optimization procedure.},
  Doi                      = {10.1109/MLSP.2010.5589003}
}

@PhdThesis{Turner2011a,
  Title                    = {{Gaussian Processes for State Space Models and Change Point Detection}},
  Author                   = {Turner, R. D.},
  School                   = {University of Cambridge},
  Year                     = {2011},

  Keywords                 = {Bayesian,Change point detection,Gaussian processes,Toeplitz,UKF,filtering,machine learning,nonparametric,smoothing,state space,statistics,time series}
}

@Article{Turner2010,
  Title                    = {{State-Space Inference and Learning with Gaussian Processes}},
  Author                   = {Turner, R. D. and Deisenroth, M. P. and Rasmussen, C. E.},
  Journal                  = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2010},
  Pages                    = {868--875},

  Annote                   = { GPIL algorithm for learning state transition dynamics without the need for groud truth observations of the latent space x.},
  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Turner, Deisenroth, Rasmussen, State-Space Inference and Learning with Gaussian Processes, 2010.pdf:pdf},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2010\_TurnerDR10.pdf}
}

@Article{Turner2009,
  Title                    = {{System identification in Gaussian process dynamical systems}},
  Author                   = {Turner, R. D. and Deisenroth, M. P. and Rasmussen, C. E.},
  Year                     = {2009},
  Pages                    = {1--3},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Turner, Deisenroth, Rasmussen, System identification in Gaussian process dynamical systems, 2009.pdf:pdf},
  Url                      = {http://eprints.pascal-network.org/archive/00006483/}
}

@InProceedings{VanderMerwe2003,
  Title                    = {{Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space Models}},
  Author                   = {{Van der Merwe}, R. and Wan, E.},
  Booktitle                = {Proceedings of the Workshop on Advances in Machine Learning},
  Year                     = {2003}
}

@InProceedings{Merwe2001,
  Title                    = {{Efficient Derivative-Free Kalman Filters for Online Learning.}},
  Author                   = {{Van der Merwe}, R. and Wan, E.},
  Booktitle                = {European Symposium on Artificial Neural Networks, ESANN},
  Year                     = {2001},
  Number                   = {April},
  Pages                    = {205--210}
}

@InCollection{Wahba1990,
  Title                    = {{Finite-Dimensional Approximating Subspaces}},
  Author                   = {Wahba, G.},
  Booktitle                = {Spline Models for Observational Data},
  Publisher                = {SIAM},
  Year                     = {1990},
  Chapter                  = {7},
  Pages                    = {95--99},
  Series                   = {CBMS-NSF Regional Conference Series in Applied Mathematics},

  Doi                      = {dx.doi.org/10.1137/1.9781611970128.ch7}
}

@Article{Wang2008a,
  Title                    = {{Gaussian process dynamical models for human motion.}},
  Author                   = {Wang, J. M. and Fleet, D. J. and Hertzmann, A.},
  Journal                  = {IEEE transactions on pattern analysis and machine intelligence},
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {283--98},
  Volume                   = {30},

  Abstract                 = {We introduce Gaussian process dynamical models (GPDM) for nonlinear time series analysis, with applications to learning models of human pose and motion from high-dimensionalmotion capture data. A GPDM is a latent variable model. It comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian process priors for both the dynamics and the observation mappings. This results in a non-parametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach, and compare four learning algorithms on human motion capture data in which each pose is 50-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces.},
  Doi                      = {10.1109/TPAMI.2007.1167},
  Keywords                 = {Algorithms,Artificial Intelligence,Biological,Biomedical Engineering,Computer Simulation,Gait,Gait: physiology,Humans,Linear Models,Models,Movement,Movement: physiology,Nonlinear Dynamics,Video Recording,Walking,Walking: physiology},
  Pmid                     = {18084059},
  Shorttitle               = {Pattern Analysis and Machine Intelligence, IEEE Tr}
}

@Article{Wang2005a,
  Title                    = {{Gaussian process dynamical models}},
  Author                   = {Wang, J. M. and Hertzmann, A. and Blei, D. M.},
  Journal                  = {Advances in neural \ldots},
  Year                     = {2005}
}

@InProceedings{Wang2014a,
  Title                    = {{Bayesian Filtering with Online Gaussian Process Latent Variable Models}},
  Author                   = {Wang, Y. and Brubaker, M. A. and Chaib-draa, B. and Urtasun, R.},
  Booktitle                = {30th Conference on Uncertainty in Artificial Intelligence},
  Year                     = {2014},
  Pages                    = {849--857}
}

@InProceedings{Wang2012,
  Title                    = {{A Marginalized Particle Gaussian Process Regression}},
  Author                   = {Wang, Y. and Chaib-draa, B.},
  Booktitle                = {Advances in Neural Information Processing Systems 25},
  Year                     = {2012},
  Editor                   = {Pereira, F. and Burges, C.J.C. and Bottou, L. and Weinberger, K.Q.},
  Pages                    = {1187--1195},
  Publisher                = {Curran Associates, Inc.},

  Abstract                 = {We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods.},
  Annote                   = {MPGP algorithm supports online learning of hyperparameters.}
}

@Book{Wasserman2007,
  Title                    = {{All of Nonparametric Statistics}},
  Author                   = {Wasserman, L.},
  Publisher                = {Springer},
  Year                     = {2007},
  Pages                    = {270},

  ISBN                     = {978-0387251455}
}

@Book{Wasserman2004,
  Title                    = {{All of Statistics: A Concise Course in Statistical Inference}},
  Author                   = {Wasserman, L.},
  Publisher                = {Springer},
  Year                     = {2004},
  Pages                    = {442},

  ISBN                     = {978-0387402727}
}

@InProceedings{Wigren2013,
  Title                    = {{Three free data sets for development and benchmarking in nonlinear system identification}},
  Author                   = {Wigren, T. and Schoukens, J.},
  Booktitle                = {European Control Conference (ECC)},
  Year                     = {2013},
  Pages                    = {2933--2938},

  File                     = {:C$\backslash$:/Users/JPruher/Documents/Mendeley Desktop/Wigren, Schoukens, Three free data sets for development and benchmarking in nonlinear system identification, 2013.pdf:pdf},
  ISBN                     = {9783033039629}
}

@InCollection{Williams1999,
  Title                    = {{Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond}},
  Author                   = {Williams, C. K. I.},
  Booktitle                = {Learning in Graphical Models},
  Publisher                = {MIT Press},
  Year                     = {1999},
  Chapter                  = {23},
  Editor                   = {Jordan, M. I.},
  Pages                    = {599--621},

  Abstract                 = {The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads in to a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions for neural network models and the use of Gaussian processes in classification problems.},
  ISBN                     = {978-0262600323}
}

@InCollection{Williams2001,
  Title                    = {{Using the Nystr\"{o}m Method to Speed Up Kernel Machines}},
  Author                   = {Williams, C. K. I. and Seeger, M.},
  Booktitle                = {Advances in Neural Information Processing Systems 13},
  Publisher                = {MIT Press},
  Year                     = {2001},
  Editor                   = {Leen, T.K. and Dietterich, T.G. and Tresp, V.},
  Pages                    = {682--688}
}

@PhdThesis{Wilson2014,
  Title                    = {{Covariance Kernels for Automatic Pattern Discovery and Extrapolation with Gaussian Processes}},
  Author                   = {Wilson, A. G.},
  School                   = {University of Cambridge},
  Year                     = {2014}
}

@InProceedings{Wilson2012a,
  Title                    = {{Gaussian process regression networks}},
  Author                   = {Wilson, A. G. and Knowles, D. A. and Ghahramani, Z.},
  Booktitle                = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  Year                     = {2012},
  Number                   = {1996},
  Pages                    = {599--606},

  Annote                   = {We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the nonparametric flexibility of Gaussian pro- cesses. GPRN accommodates input (predictor) dependent signal and noise correlations between multiple output (response) variables, input dependent lengthscales and amplitudes, and heavy-tailed predictive distributions. We derive both elliptical slice sampling and variational Bayes inference procedures for GPRN. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on real datasets, including a 1000 dimensional gene expression dataset.}
}

@Article{Wishner1969,
  Title                    = {{A comparison of three non-linear filters}},
  Author                   = {Wishner, R. P. and Tabaczynski, J. A. and Athans, M.},
  Journal                  = {Automatica},
  Year                     = {1969},
  Number                   = {4},
  Pages                    = {487--496},
  Volume                   = {5},

  Abstract                 = {This paper examines three distinct methods for the recursive estimation of the state variables of a continuous time non-linear plant on the basis of measuring the time discrete outputs of the plant in the presence of noise. The three suboptimal estimation algorithms are the Extended Kalman filter, a second-order non-linear filter, and a single stage iteration filter. The three filters are derived from the same theoretical basis in order to facilitate their comparison. Simulation results are used to compare the performance of the filters in the cases of linear plant dynamics and a non-linear output, non-linear plant dynamics and a linear output, and non-linear plant dynamics and non-linear output. We are able to conclude that the single stage iteration filter has superior mean squared error performance under all conditions, followed by the second-order filter. The second-order filter appears to be more of an unbiased estimator than the other filters. The results also show that both the single stage iteration filter and second order filter have more capability in treating non-linearities in the plant dynamics than in treating output non-linearities. Â© 1969.}
}

@Article{Wu2006,
  Title                    = {{A Numerical-Integration Perspective on Gaussian Filters}},
  Author                   = {Wu, Y. and Hu, D. and Wu, M. and Hu, X.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2006},
  Number                   = {8},
  Pages                    = {2910--2921},
  Volume                   = {54}
}

@Article{Xu2012,
  Title                    = {{Efficient Bayesian spatial prediction with mobile sensor networks using Gaussian Markov random fields}},
  Author                   = {Xu, Y. and Choi, J. and Dass, S. and Maiti, T.},
  Journal                  = {American Control Conference ( \ldots},
  Year                     = {2012},
  Number                   = {8},
  Pages                    = {2171--2176},
  Volume                   = {48},

  Abstract                 = {In this paper, we consider the problem of predicting a large scale spatial field using successive noisy measurements obtained by mobile sensing agents. The physical spatial field of interest is discretized and modeled by a Gaussian Markov random field (GMRF) with unknown hyperparameters. From a Bayesian perspective, we design a sequential prediction algorithm to exactly compute the predictive inference of the random field. The prediction algorithm correctly takes into account the uncertainty in hyperparameters in a Bayesian way and also is scalable to be usable for the mobile sensor networks with limited resources. An adaptive sampling strategy is also designed for mobile sensing agents to find the most informative locations in taking future measurements in order to minimize the prediction error and the uncertainty in hyperparameters simultaneously. The effectiveness of the proposed algorithms is illustrated by a numerical experiment.},
  Doi                      = {10.1016/j.automatica.2012.05.029},
  Keywords                 = {mobile sensor networks},
  Publisher                = {Elsevier Ltd}
}

@Article{Yeddanapudi1997,
  Title                    = {{IMM Estimation for Multitarget-Multisensor Air Traffic Surveillance}},
  Author                   = {Yeddanapudi, M. and Bar-Shalom, Y. and Pattipati, K. R.},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {80--96},
  Volume                   = {85},

  Doi                      = {10.1109/5.554210}
}

@Article{Zangl2008,
  Title                    = {{Optimal Design of Multiparameter Multisensor Systems}},
  Author                   = {H. Zangl and G. Steiner},
  Journal                  = {IEEE Transactions on Instrumentation and Measurement},
  Year                     = {2008},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1484-1491},
  Volume                   = {57},

  Abstract                 = {This paper addresses the optimal design of multiparameter multisensor systems with suboptimal estimators. For error propagation, the approach makes use of the so-called unscented transformation, which is an efficient way to determine the mean and variance of random distributions that undergo nonlinear transformations. It is demonstrated that a performance improvement can be achieved compared to the designs based on the Fisher information.},
  Doi                      = {10.1109/TIM.2008.917175},
  ISSN                     = {0018-9456},
  Keywords                 = {measurement uncertainty;parameter estimation;sensors;Fisher information;error propagation;multiparameter multisensor systems;nonlinear transformations;suboptimal estimators;unscented transformation;Error propagation;measurement uncertainty minimization;optimality criteria;suboptimal estimators;unscented transformation (UT)}
}

@Article{Zhang2014,
  Title                    = {{Quasi-Stochastic Integration Filter for Nonlinear Estimation}},
  Author                   = {Zhang, Y.-G. and Huang, Y.-L. and Wu, Z.-M. and Li, N.},
  Journal                  = {Mathematical Problems in Engineering},
  Year                     = {2014},
  Pages                    = {1--10},
  Volume                   = {2014},

  Doi                      = {10.1155/2014/967127}
}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Applications of Moment Transforms\;0\;Heine2006\;Hu200
8\;Savin2013\;Sun2014\;Zangl2008\;;
}

